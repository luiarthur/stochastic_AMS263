{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open in [nbviewer](http://nbviewer.jupyter.org/github/luiarthur/stochastic_AMS263/blob/master/notes/notes2.ipynb)\n",
    "$\n",
    "% Latex definitions\n",
    "% note: Ctrl-shfit-p for shortcuts menu\n",
    "\\newcommand{\\iid}{\\overset{iid}{\\sim}}\n",
    "\\newcommand{\\ind}{\\overset{ind}{\\sim}}\n",
    "\\newcommand{\\p}[1]{\\left(#1\\right)}\n",
    "\\newcommand{\\bk}[1]{\\left[#1\\right]}\n",
    "\\newcommand{\\bc}[1]{ \\left\\{#1\\right\\} }\n",
    "\\newcommand{\\abs}[1]{ \\left|#1\\right| }\n",
    "\\newcommand{\\ceil}[1]{ \\lceil#1\\rceil }\n",
    "\\newcommand{\\norm}[1]{ \\left|\\left|#1\\right|\\right| }\n",
    "\\newcommand{\\E}{ \\text{E} }\n",
    "\\newcommand{\\N}{ \\mathcal N }\n",
    "\\newcommand{\\ds}{ \\displaystyle }\n",
    "\\newcommand{\\R}{ \\mathbb{R} }\n",
    "\\newcommand{\\suml}{ \\sum_{i=1}^n }\n",
    "\\newcommand{\\prodl}{ \\prod_{i=1}^n }\n",
    "\\newcommand{\\overunderset}[3]{\\overset{#1}{\\underset{#2}{#3}}}\n",
    "\\newcommand{\\asym}{\\overset{\\cdot}{\\sim}}\n",
    "\\newcommand{\\given}{\\bigg |}\n",
    "\\newcommand{\\M}{\\mathcal{M}}\n",
    "\\newcommand{\\Mult}{\\text{Mult}}\n",
    "\\newcommand{\\F}{\\mathcal{F}}\n",
    "\\newcommand{\\P}{\\mathcal{P}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models\n",
    "Observation $Y_n$ for $n=0,1,...$ are generated from a conditional distribution $f(y_n|x_n)$ with parameters depending on an unnobserved or hidden state, $x_n \\in \\bc{1,2,...,K}$. Hidden states follow a transition matrix $P$.\n",
    "\n",
    "## Partially Observed Data: Inference Example\n",
    "Let the data be observed at time $t_1, t_6, t_9,t_{20},t_{35}$. Let the transition matrix be fiven by $P = \\p{p_{ij}}_{i,j=1}^m$.\n",
    "\n",
    "If the all observations were present, $\\prod_{i,j}^m p_{ij}^{n_{ij}}$. $n_{ij}$ is the number of transitions from i to j.\n",
    "\n",
    "Let $x_0$ be the known initial state and we observe $X_0=(x_{n_1},...,x_{n_m})$ where $n_1 < ... < n_m \\in \\mathbb{N}$.\n",
    "\n",
    "$$\n",
    "L(p|x_0) = \\prod_{i=1}^m p_{n_{i-1},n_i}^{t_i-t_{i-1}}\n",
    "$$\n",
    "\n",
    "where $p_{ij}^{(t)}$ is the (i,j)-th entry of $t$ step transition matrix. i.e. $P^t$.\n",
    "\n",
    "## Hidden Markov Model (HMM)\n",
    "An HMM is based upon unobserved finite state RVs $S_t \\in \\bc{1,...,m}$ which evolve according to a markov chain. i.e. \n",
    "\n",
    "$$ P(S_t=j \\mid S_{t-1}=i) = p_{ij} $$\n",
    "\n",
    "where $\\p{p_{ij}}_{i,j=1}^m$ is a transition matrix . \n",
    "\n",
    "Let $\\pi_1$ be the probability distribution of $S_1$.\n",
    "\n",
    "Assume that the chain is irreducible, aperiodic and time homogeneous. These are required for identifiability.\n",
    "\n",
    "At each observation point $t$, a realization of the state occurs. Given $S_t=k$, $y_t$ is drawn as follows:\n",
    "\n",
    "$$ y_t \\mid y_{t-1},\\theta_k \\sim f(y_t\\mid y_{t-1},\\theta_k) $$\n",
    "\n",
    "where $y_{t-1} = (y_1,...,y{t-1})$ and $k=1,...,m$.\n",
    "\n",
    "This implies that\n",
    "\n",
    "$$\n",
    "f(y_t|y_{t-1},s_{t-1},\\theta) = \n",
    "\\begin{cases}\n",
    "\\sum_{k=1}^m f(y_t \\mid y_{t-1},\\theta_k) \\pi_1(s_t=k), & t=1 \\\\\n",
    "\\sum_{k=1}^m f(y_t \\mid y_{t-1},\\theta_k) P(s_t=k|s_{t-1}), & t\\ge 2 \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{\\theta} = (\\theta_1,...,\\theta_m, p_{ij}, i,j,=1,...,m)$.\n",
    "\n",
    "This is very different from the mixture model. In a mixture model, component specific latent variables are generally independent. In HMM, there is a serial correlation b/w them.\n",
    "\n",
    "This representation is computationally cumbersome. So, we use $s_1,...,s_n$ as latent parameters and sample them alongside.\n",
    "\n",
    "Good paper to read: **[Chib (1996) HMM](../resources/chib1996.pdf)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Define: $S_t = (s_1,...,s_t)$,  $S^{t+1} = (s_{t+1},...s_n)$. Similarly, $Y_t=(y_1,...,y_t)$ and $Y^{t+1}=(y_{t+1},...,y_n)$.\n",
    "\n",
    "$$P(S_n \\mid Y_n, \\theta) = p(s_n\\mid Y_n,\\theta) \\times ... \\times p(s_t\\mid Y_n,S^{t+1},\\theta) \\times p(s_1\\mid Y_n,S^2,\\theta)$$\n",
    "\n",
    "$p(s_t \\mid Y_n, S^{t+1},\\theta)$ is a typical term in this product.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "p(s_t \\mid Y_n, S^{t+1},\\theta) &\\propto p(s_t \\mid Y_t, \\theta) ~g(Y^{t+1},S^{t+1}\\mid Y_t,s_t,\\theta) \\\\\n",
    "&\\propto p(s_t \\mid Y_t, \\theta)~ p(s_{t+1}\\mid s_t,\\theta) ~g(Y^{t+1},S^{t+2}\\mid Y_t,s_t,s_{t+1},\\theta) \\\\\n",
    "\\\\\n",
    "\\Rightarrow p(s_t \\mid Y_n, S^{t+1},\\theta) &\\propto p(s_t \\mid Y_t, \\theta)~ p(s_{t+1}\\mid s_t,\\theta)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "The last step follows because $Y^{t+1},S^{t+1}|s_{t+1}$ is independent of $s_t$ by the Markov property.\n",
    "\n",
    "Thus the mass function of $s_t$ is proportional to the product of two terms, one of which is the mass function of $s_t$ given $(Y_t,\\theta)$ and the other is the transition prob given $\\theta$.\n",
    "\n",
    "Assume $p(s_{t-1}\\mid Y_{t-1},\\theta)$ is available, then repeat the following steps:\n",
    "\n",
    "### Prediction step\n",
    "$$ p(s_t|Y_{t-1},\\theta) = \\sum_{k=1}^m p(s_t|s_{t-1}=k,\\theta) p(s_{t-1}=k|Y_{t-1},\\theta)$$\n",
    "\n",
    "### UPdate step\n",
    "$$ p(s_t|Y_{t},\\theta) \\propto p(s_t|Y_{t-1}=k,\\theta) f(y_{t}|Y_{t-1},\\theta)$$\n",
    "\n",
    "\n",
    "### Algorithm\n",
    "Initialize at $t=1$ by setting $p(s_1|y_0,\\theta)$ to be the stationary distribution of the chain.\n",
    "\n",
    "Run the prediction and update steps recursively ro comp[ute the mass fn of $p(s_t|Y_t,\\theta)$. \n",
    "\n",
    "$S_n$ is the first updated Then the remaining steps are simulated from equation (1) above...\n",
    "\n",
    "We know how to draw samples from $s_1,...,s_n$. \n",
    "\n",
    "### p-update\n",
    "WE use $p_i=(p_{i1},...,p_{im})\\sim Dir(\\alpha_{i1},...,alpha_{im})$ then $p_i mid s_n \\sim Dir(\\alpha_{i1}+n_{i1},...,\\alpha_{im}+n_{im})$, where $n_{ik}$ = the total number of transitions $i$ to $k$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See Example in Chib 1996 Section 4.1\n",
    "\n",
    "Infact, just refer to the paper for this lecture on HMM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TO DO\n",
    "\n",
    "- choose project by 3 March.\n",
    "    - 20 minutes\n",
    "- make-up class tomorrow 1-2 pm. 246 Porter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point Processes\n",
    "\n",
    "Point Processes are SP for events that occur separated in time or space.\n",
    "\n",
    "If points are independently distributed, we would expect that the location of each point is independent of the location of other points. But there can be certain pattern of points. We would like to model that.\n",
    "\n",
    "Poisson process plays an important  role in the study of point processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-homogeneous Poisson Process (NHPP)\n",
    "\n",
    "NHPP are defined on the observation window $R$ with intensity $lambda(x), x \\in R$ which is a non-negative and lcally integrable function for all bounded $B \\subset R$, the following holds:\n",
    "\n",
    "1. for any $B$, the number of points in $B$, $N(B) \\sim Pois(\\Lambda(B))$, where $\\Lambda(B) = \\int_B\\lambda(x) dx$\n",
    "2. Given $N(B)$, the point locations within $B$ are iid with density $\\frac{\\lambda(x)}{\\int_B\\lambda(x) dx}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider first NHPP in 1-dim. Spatial NHPP (in 2-dim) will follow later.\n",
    "\n",
    "Let us study NHPP in the interval $R = (0,1)$ with events occuring at points $0 < t_1 < t_2<...<t_N<1$.\n",
    "\n",
    "$P(N \\text{ events occur in} (0,1)) = \\frac{e^{-\\Lambda(B)}\\Lambda(B)^N}{N!}$, where $\\Lambda(B) = \\int_0^1 \\lambda(x) dx$\n",
    "\n",
    "by (2), $P(\\text{events happened at} t_1<t_2<...<t_N | N \\text{events}) = \\prod_{i=1}^N \\frac{\\lambda(t_i)}{\\int_B\\lambda(x) dx}$\n",
    "\n",
    "$P(N \\text{events happened at points} t_1<...<t_N) = e^{-\\Lambda}\\frac{\\prod_{i=1}^N\\lambda(t_i)}{N!}$\n",
    "\n",
    "### Prior on $\\lambda(t)$\n",
    "\n",
    "1. assume some parametric form of $\\lambda(t)$ and put priors on parameters\n",
    "2. assume fully non-parametric prior on $\\lambda(t)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric method:\n",
    "\n",
    "Assume $\\lambda(t)=\\alpha t^{-\\beta}$, for $\\alpha>0, \\beta\\in \\mathbb{R}$\n",
    "\n",
    "### Nonparametric Prior\n",
    "\n",
    "Define $f(t)=\\frac{\\lambda(t)}{\\nu}, \\nu = \\int_0^1\\lambda(u)du$\n",
    "\n",
    "$f(t)$ is a density function on (0,1). $(f,\\nu)$ provides an equivalent representation of $\\lambda$. So a nonparametric prior for $f$ with a parametric prior on $\\nu$ will induce a semi-parametric prior on $\\lambda$.\n",
    "\n",
    "$\\nu$ determines the scale and $f$ will determines the shape of $\\lambda$.\n",
    "\n",
    "There are two different non-parametric priors that one can think of in estimating $f$.\n",
    "\n",
    "1. DP mixture prior\n",
    "    - $f(t) = \\int \\text{Beta}(t; \\mu,\\tau) dG(\\mu,\\tau)$, where $\\mu \\in (0,1)$ and scale parameter $\\tau>0$\n",
    "    - $G \\sim DP(\\alpha, G_0)$\n",
    "    - DP Books References:\n",
    "        - Dey, Muller, Sinha (1998)\n",
    "        - Gosh & Rammoonorti (2003)\n",
    "        - hjort, Holmes, Muller, Walker (2010)\n",
    "        - Muller & Rodriguex (2013)\n",
    "2. Logistic GP prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model intensity function of $\\lambda(t)$ in a non-homogeneous Poisson Process\n",
    "\n",
    "$f(t) = \\frac{\\lambda(t)}{v}, v = \\int_0^1\\lambda(t)dt$\n",
    "\n",
    "We want to put prior on $\\lambda(t)$ but we instead (equivalently) put prior on $v$ and $f(t)$.\n",
    "\n",
    "$f(t) = \\int \\text{ Beta}(t;\\mu,\\tau) dG(\\mu,\\tau)$, where $\\mu\\in(0,1)$ and $\\tau>0$.\n",
    "\n",
    "$G\\sim DP(\\alpha,G_0)$ $G_0(\\mu,\\tau) = G_{01}(\\mu)G_{02}(\\tau)$ we take the base distribution for $\\mu$ to be uniform (0,1) and take the base distribution for $\\tau$ to be gamma(a,b).\n",
    "\n",
    "prior on $v$: $p(v)=1/v$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Likelihood:**  $e^{-\\Lambda}\\frac{\\prod_{i=1}^N\\lambda(t_i)}{N!}$\n",
    "which is proportional to $e^{-v} \\prodl\\bc{f(t_i)v}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Logistic Gaussian Process Prior is used To Estimate $f(t)$\n",
    "\n",
    "Tokdai et at (2007)\n",
    "\n",
    "Take $I=[0,1]$. We are interested in estimating a density that is defined over $[0,1]$. Let $\\sigma_0(.,.)$\n",
    "be a fixed positive definite function on $\\mathbb{R}\\times\\mathbb{R}$.\n",
    "If you take $t_1,...,t_m$ for any $m$, \n",
    "\n",
    "$\\Sigma = (((\\sigma_0(t_i,t_j))))_{i,j=1}^m$ is a positive definite matrix.\n",
    "\n",
    "Define a real valued process $f_N$ on $I$ as follows, \n",
    "\n",
    "$f_N(t) = \\ds\\frac{e^{W(t)}}{\\int_I e^{W(t)}ds}$, $t\\in I$, where given $\\gamma=(\\tau,\\beta) \\in \\mathbb{R}^+\\times \\mathbb{R}^+$.\n",
    "\n",
    "$W\\sim GP(0,\\sigma_\\gamma(s,t))$, $\\sigma(s,t)=\\tau^2\\sigma_0(\\beta s, \\beta t)$. The prior on $f$ is going to generate realizations of $f$ of the form $f_W$. And of course, $\\int f_W(t) dt = 1$. This prior is called the logistic Gaussian process prior.\n",
    "\n",
    "Small values of $\\beta$ results in smooth sample paths, while large $\\beta$ produces oscillating sample paths.\n",
    "$\\tau$ controls variability of $f_W$ from its prior.\n",
    "\n",
    "The posterior distribution given observations at points $t_1,...,t_n$ is given by \n",
    "$\\ds e^{-\\nu}\\bc{\\prodl \\nu f_w(t_i)} \\times \\pi(\\beta) \\pi(\\tau^2) \\times \\N\\p{(W(t_1),...,W(t_n))'\\mid 0,\\Sigma}$\n",
    "\n",
    "When the number of points $n$ is large, the MCMC becomes prohibitive as it requires inverting the matrix $\\Sigma$ in every iteration.\n",
    "\n",
    "Computational issues can be solved by imputations. $T=\\bc{x_1,...,x_m} \\subset S$. We approximate $W$ by a new process $z(t)=E[W(t)|W_m,\\gamma], t\\in I$ , where $W_m=(W(x_1),...,W(x_m))$. \n",
    "This gives us \n",
    "\n",
    "$z(t) = W_m'\\Sigma_\\gamma^{-1}\\sigma_\\gamma(t), \\Sigma_\\gamma=((\\sigma_\\gamma(x_i,x_j)))_{i,j}^m$.\n",
    "\n",
    "$\\sigma_\\gamma(t) = (\\sigma_\\gamma(x_1,t),...,\\sigma_\\gamma(x_m,t))$.\n",
    "\n",
    "$\\ds f_W(t) = f_{X^TA_\\gamma}(t) = \\frac{\\exp\\p{X^T A\\gamma(t)}}{\\int_0^1 \\exp\\p{X^T A\\gamma(s)} ds}$,\n",
    "\n",
    "where $X\\sim \\N_m(0,I)$ and $A_\\gamma(t) = \\Sigma_\\gamma^{-1/2} \\sigma_\\gamma(t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "put prior on $\\gamma=(\\beta,\\tau^2)$. Call the prior on $\\gamma$ as $H(\\gamma)$.\n",
    "\n",
    "Given $\\gamma$, we have the distributioon of $W_m$ is normal. And hence $z(t)$ has the same dist as $X^TA_\\gamma(t)$. This approx of $W(t)$ by $z(t)$ is useful if posterior dist of $f_w|y$ is well approxmiated by $f_z|y$.\n",
    "\n",
    "Let $\\widehat{f_W} = \\E\\bk{f_W|y}$, and $\\widehat{f_Z} = \\E\\bk{f_Z|y}$.\n",
    "Then assume that there exists positive $c,q$ such that for all $s,t\\in \\mathbb{R}$, and $\\gamma\\in\\sup(H)$\n",
    "\n",
    "$ \\sqrt{Var(W(s)-W(t))} \\le c\\norm{s-t}^q $, let $\\delta(T) = \\sup \\min\\norm{t_i-t_j}$. $t$ in unity\n",
    "\n",
    "$\\delta(T)$ is called the fitness o nodes. then $KL(\\hat{f_W},\\hat{f_Z})\\rightarrow 0$ as $\\delta(T)\\rightarrow 0$.\n",
    "\n",
    "### Computation\n",
    "Given data $(y_1,...,y_n)$, the posterior density of $(X,\\gamma)$ can be written as \n",
    "\n",
    "$$ p(X,\\gamma | y) \\propto \\bc{\\prodl f_X^TA_\\gamma(y_j)} \\N(X|0,I_m) \\times H(\\gamma) $$\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1. Initialize\n",
    "2. propose to move from $X$ to $X'=(x_1',x_2,...,x_m)$.  $x_1'$ is generated from $\\N(x_1,\\sigma_{x_1}^2)$. Use MH.\n",
    "    - Accept the new move with prob $\\alpha = \\ds\\min\\bc{1,\\frac{\\phi_1(x_1')}{\\phi_1(x_1)}\\frac{\\prodl f(x')^TA_\\gamma(y_j)}{\\prodl f(x)^TA_\\gamma(y_j)}}$.\n",
    "    - (do this for each $x_j$)\n",
    "3. update $\\gamma$ using Metropolis\n",
    "    - recall $f_{X^t A_\\gamma(t)} = ...$ (see above) which requires the computation of an integral.\n",
    "    - one can evaluate the process on a very fine grid. Let $G\\subset [0,1]$ be the grid. The integral can just be numerically approximated (with the area). Do this at every MCMC iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Gaussian Cox Process\n",
    "\n",
    "Doubly Stochastic Poisson Process.\n",
    "\n",
    "In Poisson Process, the intensity fn is an unknown but fixed function $\\lambda(t)$. In log Gaussian Cox process, \n",
    "$\\lambda(t)$ is assumed to be a random function.\n",
    "\n",
    "Both the processes $Y(t)$ and the intensity fn $\\Lambda=\\bc{\\lambda(t):t\\in\\mathbb{R}}$  are SPs.\n",
    "\n",
    "We assume $y|\\Lambda \\sim $ Poisson process with intensity $\\lambda$.\n",
    "\n",
    "In general, one restricts attention to cases where $\\Lambda$ and hence $y$ is starionary, and sometimes, also\n",
    "isotropic. One models $\\lambda(s)$ using a log GP.\n",
    "\n",
    "$\\lambda(s) = \\exp(Z(s))$ where $Z=\\bc{z(s): s\\in \\mathbb{R}}$ is a real valued GP. $log(\\lambda(s)) = z(s)$.\n",
    "If there are a number of predictors, they can be incorporated in the above equation using \n",
    "$\\log\\lambda(s) = z(s) + x(s)'\\beta$, where $x(s)$ is the predictor.\n",
    "\n",
    "Also, before people have worked on replacing the GP $Z(s)$ by a predictive process or kernel convolution for computational tractability.\n",
    "\n",
    "### Likelihood of log-Gaussian Cox process\n",
    "Let the obs be found at locations $t_1,...,t_n$. The likelihood is given by \n",
    "$\\E_\\lambda\\bk{\\exp\\bc{-\\int_0^1 \\lambda(s) ds} \\prodl \\lambda(t_i)}$\n",
    "\n",
    "We know $\\log \\lambda(t) = z(t)$ which follows a GP. We have to calculate joint dist of $(\\lambda(t_1),...,\\lambda(t_n))$, which is known as the joint dist from a log Gaussian Cox proc.\n",
    "\n",
    "One can also simplify this a bit by assuming the sparse GP on $Z$ so that the expectation is always over \n",
    "$(z(x_1),...,z(x_m))$ where $\\bc{x_1,...,x_m}$ are knot points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
