{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open in [nbviewer](http://nbviewer.jupyter.org/github/luiarthur/stochastic_AMS263/blob/master/notes/notes2.ipynb)\n",
    "$\n",
    "% Latex definitions\n",
    "% note: Ctrl-shfit-p for shortcuts menu\n",
    "\\newcommand{\\iid}{\\overset{iid}{\\sim}}\n",
    "\\newcommand{\\ind}{\\overset{ind}{\\sim}}\n",
    "\\newcommand{\\p}[1]{\\left(#1\\right)}\n",
    "\\newcommand{\\bk}[1]{\\left[#1\\right]}\n",
    "\\newcommand{\\bc}[1]{ \\left\\{#1\\right\\} }\n",
    "\\newcommand{\\abs}[1]{ \\left|#1\\right| }\n",
    "\\newcommand{\\ceil}[1]{ \\lceil#1\\rceil }\n",
    "\\newcommand{\\norm}[1]{ \\left|\\left|#1\\right|\\right| }\n",
    "\\newcommand{\\E}{ \\text{E} }\n",
    "\\newcommand{\\N}{ \\mathcal N }\n",
    "\\newcommand{\\ds}{ \\displaystyle }\n",
    "\\newcommand{\\R}{ \\mathbb{R} }\n",
    "\\newcommand{\\suml}{ \\sum_{i=1}^n }\n",
    "\\newcommand{\\prodl}{ \\prod_{i=1}^n }\n",
    "\\newcommand{\\overunderset}[3]{\\overset{#1}{\\underset{#2}{#3}}}\n",
    "\\newcommand{\\asym}{\\overset{\\cdot}{\\sim}}\n",
    "\\newcommand{\\given}{\\bigg |}\n",
    "\\newcommand{\\M}{\\mathcal{M}}\n",
    "\\newcommand{\\Mult}{\\text{Mult}}\n",
    "\\newcommand{\\F}{\\mathcal{F}}\n",
    "\\newcommand{\\P}{\\mathcal{P}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models\n",
    "Observation $Y_n$ for $n=0,1,...$ are generated from a conditional distribution $f(y_n|x_n)$ with parameters depending on an unnobserved or hidden state, $x_n \\in \\bc{1,2,...,K}$. Hidden states follow a transition matrix $P$.\n",
    "\n",
    "## Partially Observed Data: Inference Example\n",
    "Let the data be observed at time $t_1, t_6, t_9,t_{20},t_{35}$. Let the transition matrix be fiven by $P = \\p{p_{ij}}_{i,j=1}^m$.\n",
    "\n",
    "If the all observations were present, $\\prod_{i,j}^m p_{ij}^{n_{ij}}$. $n_{ij}$ is the number of transitions from i to j.\n",
    "\n",
    "Let $x_0$ be the known initial state and we observe $X_0=(x_{n_1},...,x_{n_m})$ where $n_1 < ... < n_m \\in \\mathbb{N}$.\n",
    "\n",
    "$$\n",
    "L(p|x_0) = \\prod_{i=1}^m p_{n_{i-1},n_i}^{t_i-t_{i-1}}\n",
    "$$\n",
    "\n",
    "where $p_{ij}^{(t)}$ is the (i,j)-th entry of $t$ step transition matrix. i.e. $P^t$.\n",
    "\n",
    "## Hidden Markov Model (HMM)\n",
    "An HMM is based upon unobserved finite state RVs $S_t \\in \\bc{1,...,m}$ which evolve according to a markov chain. i.e. \n",
    "\n",
    "$$ P(S_t=j \\mid S_{t-1}=i) = p_{ij} $$\n",
    "\n",
    "where $\\p{p_{ij}}_{i,j=1}^m$ is a transition matrix . \n",
    "\n",
    "Let $\\pi_1$ be the probability distribution of $S_1$.\n",
    "\n",
    "Assume that the chain is irreducible, aperiodic and time homogeneous. These are required for identifiability.\n",
    "\n",
    "At each observation point $t$, a realization of the state occurs. Given $S_t=k$, $y_t$ is drawn as follows:\n",
    "\n",
    "$$ y_t \\mid y_{t-1},\\theta_k \\sim f(y_t\\mid y_{t-1},\\theta_k) $$\n",
    "\n",
    "where $y_{t-1} = (y_1,...,y{t-1})$ and $k=1,...,m$.\n",
    "\n",
    "This implies that\n",
    "\n",
    "$$\n",
    "f(y_t|y_{t-1},s_{t-1},\\theta) = \n",
    "\\begin{cases}\n",
    "\\sum_{k=1}^m f(y_t \\mid y_{t-1},\\theta_k) \\pi_1(s_t=k), & t=1 \\\\\n",
    "\\sum_{k=1}^m f(y_t \\mid y_{t-1},\\theta_k) P(s_t=k|s_{t-1}), & t\\ge 2 \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{\\theta} = (\\theta_1,...,\\theta_m, p_{ij}, i,j,=1,...,m)$.\n",
    "\n",
    "This is very different from the mixture model. In a mixture model, component specific latent variables are generally independent. In HMM, there is a serial correlation b/w them.\n",
    "\n",
    "This representation is computationally cumbersome. So, we use $s_1,...,s_n$ as latent parameters and sample them alongside.\n",
    "\n",
    "Good paper to read: **[Chib (1996) HMM](../resources/chib1996.pdf)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Define: $S_t = (s_1,...,s_t)$,  $S^{t+1} = (s_{t+1},...s_n)$. Similarly, $Y_t=(y_1,...,y_t)$ and $Y^{t+1}=(y_{t+1},...,y_n)$.\n",
    "\n",
    "$$P(S_n \\mid Y_n, \\theta) = p(s_n\\mid Y_n,\\theta) \\times ... \\times p(s_t\\mid Y_n,S^{t+1},\\theta) \\times p(s_1\\mid Y_n,S^2,\\theta)$$\n",
    "\n",
    "$p(s_t \\mid Y_n, S^{t+1},\\theta)$ is a typical term in this product.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "p(s_t \\mid Y_n, S^{t+1},\\theta) &\\propto p(s_t \\mid Y_t, \\theta) ~g(Y^{t+1},S^{t+1}\\mid Y_t,s_t,\\theta) \\\\\n",
    "&\\propto p(s_t \\mid Y_t, \\theta)~ p(s_{t+1}\\mid s_t,\\theta) ~g(Y^{t+1},S^{t+2}\\mid Y_t,s_t,s_{t+1},\\theta) \\\\\n",
    "\\\\\n",
    "\\Rightarrow p(s_t \\mid Y_n, S^{t+1},\\theta) &\\propto p(s_t \\mid Y_t, \\theta)~ p(s_{t+1}\\mid s_t,\\theta)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "The last step follows because $Y^{t+1},S^{t+1}|s_{t+1}$ is independent of $s_t$ by the Markov property.\n",
    "\n",
    "Thus the mass function of $s_t$ is proportional to the product of two terms, one of which is the mass function of $s_t$ given $(Y_t,\\theta)$ and the other is the transition prob given $\\theta$.\n",
    "\n",
    "Assume $p(s_{t-1}\\mid Y_{t-1},\\theta)$ is available, then repeat the following steps:\n",
    "\n",
    "### Prediction step\n",
    "$$ p(s_t|Y_{t-1},\\theta) = \\sum_{k=1}^m p(s_t|s_{t-1}=k,\\theta) p(s_{t-1}=k|Y_{t-1},\\theta)$$\n",
    "\n",
    "### UPdate step\n",
    "$$ p(s_t|Y_{t},\\theta) \\propto p(s_t|Y_{t-1}=k,\\theta) f(y_{t}|Y_{t-1},\\theta)$$\n",
    "\n",
    "\n",
    "### Algorithm\n",
    "Initialize at $t=1$ by setting $p(s_1|y_0,\\theta)$ to be the stationary distribution of the chain.\n",
    "\n",
    "Run the prediction and update steps recursively ro comp[ute the mass fn of $p(s_t|Y_t,\\theta)$. \n",
    "\n",
    "$S_n$ is the first updated Then the remaining steps are simulated from equation (1) above...\n",
    "\n",
    "We know how to draw samples from $s_1,...,s_n$. \n",
    "\n",
    "### p-update\n",
    "WE use $p_i=(p_{i1},...,p_{im})\\sim Dir(\\alpha_{i1},...,alpha_{im})$ then $p_i mid s_n \\sim Dir(\\alpha_{i1}+n_{i1},...,\\alpha_{im}+n_{im})$, where $n_{ik}$ = the total number of transitions $i$ to $k$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See Example in Chib 1996 Section 4.1\n",
    "\n",
    "Infact, just refer to the paper for this lecture on HMM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TO DO\n",
    "\n",
    "- choose project by 3 March.\n",
    "    - 20 minutes\n",
    "- make-up class tomorrow 1-2 pm. 246 Porter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point Processes\n",
    "\n",
    "Point Processes are SP for events that occur separated in time or space.\n",
    "\n",
    "If points are independently distributed, we would expect that the location of each point is independent of the location of other points. But there can be certain pattern of points. We would like to model that.\n",
    "\n",
    "Poisson process plays an important  role in the study of point processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-homogeneous Poisson Process (NHPP)\n",
    "\n",
    "NHPP are defined on the observation window $R$ with intensity $lambda(x), x \\in R$ which is a non-negative and lcally integrable function for all bounded $B \\subset R$, the following holds:\n",
    "\n",
    "1. for any $B$, the number of points in $B$, $N(B) \\sim Pois(\\Lambda(B))$, where $\\Lambda(B) = \\int_B\\lambda(x) dx$\n",
    "2. Given $N(B)$, the point locations within $B$ are iid with density $\\frac{\\lambda(x)}{\\int_B\\lambda(x) dx}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider first NHPP in 1-dim. Spatial NHPP (in 2-dim) will follow later.\n",
    "\n",
    "Let us study NHPP in the interval $R = (0,1)$ with events occuring at points $0 < t_1 < t_2<...<t_N<1$.\n",
    "\n",
    "$P(N \\text{ events occur in} (0,1)) = \\frac{e^{-\\Lambda(B)}\\Lambda(B)^N}{N!}$, where $\\Lambda(B) = \\int_0^1 \\lambda(x) dx$\n",
    "\n",
    "by (2), $P(\\text{events happened at} t_1<t_2<...<t_N | N \\text{events}) = \\prod_{i=1}^N \\frac{\\lambda(t_i)}{\\int_B\\lambda(x) dx}$\n",
    "\n",
    "$P(N \\text{events happened at points} t_1<...<t_N) = e^{-\\Lambda}\\frac{\\prod_{i=1}^N\\lambda(t_i)}{N!}$\n",
    "\n",
    "### Prior on $\\lambda(t)$\n",
    "\n",
    "1. assume some parametric form of $\\lambda(t)$ and put priors on parameters\n",
    "2. assume fully non-parametric prior on $\\lambda(t)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric method:\n",
    "\n",
    "Assume $\\lambda(t)=\\alpha t^{-\\beta}$, for $\\alpha>0, \\beta\\in \\mathbb{R}$\n",
    "\n",
    "### Nonparametric Prior\n",
    "\n",
    "Define $f(t)=\\frac{\\lambda(t)}{\\nu}, \\nu = \\int_0^1\\lambda(u)du$\n",
    "\n",
    "$f(t)$ is a density function on (0,1). $(f,\\nu)$ provides an equivalent representation of $\\lambda$. So a nonparametric prior for $f$ with a parametric prior on $\\nu$ will induce a semi-parametric prior on $\\lambda$.\n",
    "\n",
    "$\\nu$ determines the scale and $f$ will determines the shape of $\\lambda$.\n",
    "\n",
    "There are two different non-parametric priors that one can think of in estimating $f$.\n",
    "\n",
    "1. DP mixture prior\n",
    "    - $f(t) = \\int \\text{Beta}(t; \\mu,\\tau) dG(\\mu,\\tau)$, where $\\mu \\in (0,1)$ and scale parameter $\\tau>0$\n",
    "    - $G \\sim DP(\\alpha, G_0)$\n",
    "    - DP Books References:\n",
    "        - Dey, Muller, Sinha (1998)\n",
    "        - Gosh & Rammoonorti (2003)\n",
    "        - hjort, Holmes, Muller, Walker (2010)\n",
    "        - Muller & Rodriguex (2013)\n",
    "2. Logistic GP prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model intensity function of $\\lambda(t)$ in a non-homogeneous Poisson Process\n",
    "\n",
    "$f(t) = \\frac{\\lambda(t)}{v}, v = \\int_0^1\\lambda(t)dt$\n",
    "\n",
    "We want to put prior on $\\lambda(t)$ but we instead (equivalently) put prior on $v$ and $f(t)$.\n",
    "\n",
    "$f(t) = \\int \\text{ Beta}(t;\\mu,\\tau) dG(\\mu,\\tau)$, where $\\mu\\in(0,1)$ and $\\tau>0$.\n",
    "\n",
    "$G\\sim DP(\\alpha,G_0)$ $G_0(\\mu,\\tau) = G_{01}(\\mu)G_{02}(\\tau)$ we take the base distribution for $\\mu$ to be uniform (0,1) and take the base distribution for $\\tau$ to be gamma(a,b).\n",
    "\n",
    "prior on $v$: $p(v)=1/v$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Likelihood:**  $e^{-\\Lambda}\\frac{\\prod_{i=1}^N\\lambda(t_i)}{N!}$\n",
    "which is proportional to $e^{-v} \\prodl\\bc{f(t_i)v}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Logistic Gaussian Process Prior is used To Estimate $f(t)$\n",
    "\n",
    "Tokdai et at (2007)\n",
    "\n",
    "Take $I=[0,1]$. We are interested in estimating a density that is defined over $[0,1]$. Let $\\sigma_0(.,.)$\n",
    "be a fixed positive definite function on $\\mathbb{R}\\times\\mathbb{R}$.\n",
    "If you take $t_1,...,t_m$ for any $m$, \n",
    "\n",
    "$\\Sigma = (((\\sigma_0(t_i,t_j))))_{i,j=1}^m$ is a positive definite matrix.\n",
    "\n",
    "Define a real valued process $f_N$ on $I$ as follows, \n",
    "\n",
    "$f_N(t) = \\ds\\frac{e^{W(t)}}{\\int_I e^{W(t)}ds}$, $t\\in I$, where given $\\gamma=(\\tau,\\beta) \\in \\mathbb{R}^+\\times \\mathbb{R}^+$.\n",
    "\n",
    "$W\\sim GP(0,\\sigma_\\gamma(s,t))$, $\\sigma(s,t)=\\tau^2\\sigma_0(\\beta s, \\beta t)$. The prior on $f$ is going to generate realizations of $f$ of the form $f_W$. And of course, $\\int f_W(t) dt = 1$. This prior is called the logistic Gaussian process prior.\n",
    "\n",
    "Small values of $\\beta$ results in smooth sample paths, while large $\\beta$ produces oscillating sample paths.\n",
    "$\\tau$ controls variability of $f_W$ from its prior.\n",
    "\n",
    "The posterior distribution given observations at points $t_1,...,t_n$ is given by \n",
    "$\\ds e^{-\\nu}\\bc{\\prodl \\nu f_w(t_i)} \\times \\pi(\\beta) \\pi(\\tau^2) \\times \\N\\p{(W(t_1),...,W(t_n))'\\mid 0,\\Sigma}$\n",
    "\n",
    "When the number of points $n$ is large, the MCMC becomes prohibitive as it requires inverting the matrix $\\Sigma$ in every iteration.\n",
    "\n",
    "Computational issues can be solved by imputations. $T=\\bc{x_1,...,x_m} \\subset S$. We approximate $W$ by a new process $z(t)=E[W(t)|W_m,\\gamma], t\\in I$ , where $W_m=(W(x_1),...,W(x_m))$. \n",
    "This gives us \n",
    "\n",
    "$z(t) = W_m'\\Sigma_\\gamma^{-1}\\sigma_\\gamma(t), \\Sigma_\\gamma=((\\sigma_\\gamma(x_i,x_j)))_{i,j}^m$.\n",
    "\n",
    "$\\sigma_\\gamma(t) = (\\sigma_\\gamma(x_1,t),...,\\sigma_\\gamma(x_m,t))$.\n",
    "\n",
    "$\\ds f_W(t) = f_{X^TA_\\gamma}(t) = \\frac{\\exp\\p{X^T A\\gamma(t)}}{\\int_0^1 \\exp\\p{X^T A\\gamma(s)} ds}$,\n",
    "\n",
    "where $X\\sim \\N_m(0,I)$ and $A_\\gamma(t) = \\Sigma_\\gamma^{-1/2} \\sigma_\\gamma(t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "put prior on $\\gamma=(\\beta,\\tau^2)$. Call the prior on $\\gamma$ as $H(\\gamma)$.\n",
    "\n",
    "Given $\\gamma$, we have the distributioon of $W_m$ is normal. And hence $z(t)$ has the same dist as $X^TA_\\gamma(t)$. This approx of $W(t)$ by $z(t)$ is useful if posterior dist of $f_w|y$ is well approxmiated by $f_z|y$.\n",
    "\n",
    "Let $\\widehat{f_W} = \\E\\bk{f_W|y}$, and $\\widehat{f_Z} = \\E\\bk{f_Z|y}$.\n",
    "Then assume that there exists positive $c,q$ such that for all $s,t\\in \\mathbb{R}$, and $\\gamma\\in\\sup(H)$\n",
    "\n",
    "$ \\sqrt{Var(W(s)-W(t))} \\le c\\norm{s-t}^q $, let $\\delta(T) = \\sup \\min\\norm{t_i-t_j}$. $t$ in unity\n",
    "\n",
    "$\\delta(T)$ is called the fitness o nodes. then $KL(\\hat{f_W},\\hat{f_Z})\\rightarrow 0$ as $\\delta(T)\\rightarrow 0$.\n",
    "\n",
    "### Computation\n",
    "Given data $(y_1,...,y_n)$, the posterior density of $(X,\\gamma)$ can be written as \n",
    "\n",
    "$$ p(X,\\gamma | y) \\propto \\bc{\\prodl f_X^TA_\\gamma(y_j)} \\N(X|0,I_m) \\times H(\\gamma) $$\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1. Initialize\n",
    "2. propose to move from $X$ to $X'=(x_1',x_2,...,x_m)$.  $x_1'$ is generated from $\\N(x_1,\\sigma_{x_1}^2)$. Use MH.\n",
    "    - Accept the new move with prob $\\alpha = \\ds\\min\\bc{1,\\frac{\\phi_1(x_1')}{\\phi_1(x_1)}\\frac{\\prodl f(x')^TA_\\gamma(y_j)}{\\prodl f(x)^TA_\\gamma(y_j)}}$.\n",
    "    - (do this for each $x_j$)\n",
    "3. update $\\gamma$ using Metropolis\n",
    "    - recall $f_{X^t A_\\gamma(t)} = ...$ (see above) which requires the computation of an integral.\n",
    "    - one can evaluate the process on a very fine grid. Let $G\\subset [0,1]$ be the grid. The integral can just be numerically approximated (with the area). Do this at every MCMC iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Gaussian Cox Process\n",
    "\n",
    "Doubly Stochastic Poisson Process.\n",
    "\n",
    "In Poisson Process, the intensity fn is an unknown but fixed function $\\lambda(t)$. In log Gaussian Cox process, \n",
    "$\\lambda(t)$ is assumed to be a random function.\n",
    "\n",
    "Both the processes $Y(t)$ and the intensity fn $\\Lambda=\\bc{\\lambda(t):t\\in\\mathbb{R}}$  are SPs.\n",
    "\n",
    "We assume $y|\\Lambda \\sim $ Poisson process with intensity $\\lambda$.\n",
    "\n",
    "In general, one restricts attention to cases where $\\Lambda$ and hence $y$ is starionary, and sometimes, also\n",
    "isotropic. One models $\\lambda(s)$ using a log GP.\n",
    "\n",
    "$\\lambda(s) = \\exp(Z(s))$ where $Z=\\bc{z(s): s\\in \\mathbb{R}}$ is a real valued GP. $log(\\lambda(s)) = z(s)$.\n",
    "If there are a number of predictors, they can be incorporated in the above equation using \n",
    "$\\log\\lambda(s) = z(s) + x(s)'\\beta$, where $x(s)$ is the predictor.\n",
    "\n",
    "Also, before people have worked on replacing the GP $Z(s)$ by a predictive process or kernel convolution for computational tractability.\n",
    "\n",
    "### Likelihood of log-Gaussian Cox process\n",
    "Let the obs be found at locations $t_1,...,t_n$. The likelihood is given by \n",
    "$\\E_\\lambda\\bk{\\exp\\bc{-\\int_0^1 \\lambda(s) ds} \\prodl \\lambda(t_i)}$\n",
    "\n",
    "We know $\\log \\lambda(t) = z(t)$ which follows a GP. We have to calculate joint dist of $(\\lambda(t_1),...,\\lambda(t_n))$, which is known as the joint dist from a log Gaussian Cox proc.\n",
    "\n",
    "One can also simplify this a bit by assuming the sparse GP on $Z$ so that the expectation is always over \n",
    "$(z(x_1),...,z(x_m))$ where $\\bc{x_1,...,x_m}$ are knot points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Inference with non-homogeneous Poisson Process\n",
    "\n",
    "1. $\\Lambda = \\bc{\\lambda(x): x\\in R^2}$ is a non-negative SP. Specifically, we take $\\Lambda(x) = e^{s(x}$, and $s(x)$ follows a GP.\n",
    "2. Conditional on the $\\Lambda$, the points are distributed according to a non-homogeneous Poisson Process.\n",
    "\n",
    "Remark: If the GP $s(x)$ is stationary, then the log gaussian process is also stationary.\n",
    "\n",
    "$s(x) \\sim GP(\\mu,p(.,.)), cov(s(x),s(x-u)) = \\sigma^2 r(u) = C(u)$.  Clearly, $E[e^{s(x)}] = e^{\\mu+\\sigma^2/2}$\n",
    "\n",
    "and $cov(e^{s(x)}, e^{s(x-u)}) = \\exp(\\mu+\\sigma^2/2)\\bk{\\exp(\\sigma^2r(u))-1}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation of the parameters in Cox Process\n",
    "\n",
    "Let the data be obtained at locations $x_1,...,x_n$. Let $X = \\bc{x_i\\in A: i=1,...,n}$.\n",
    "\n",
    "Let $\\theta$ be the set of parameters of interest. They can be parameters in the GP $s(x)$, or some other parameters.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "L(\\theta; x) &\\propto P(X \\mid \\theta) = \\int P(X,\\Lambda \\mid \\theta) d\\lambda \\\\\n",
    "&\\propto \\E_{\\Lambda\\mid \\theta}\\bk{l^*(\\Lambda,x)} \\\\\n",
    "l^*(\\Lambda,x) &= \\prodl \\Lambda(x_i) \\p{\\int \\Lambda(x) dx}^{-n}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Let $\\lambda^{(j)} = \\bc{\\lambda^{(j)}(g_k): k=1,...,N}$; $j=1,...,s$ are simulated realizations of $\\Lambda$ on the set of grid points $g_k$.\n",
    "\n",
    "Let $f(x,\\Lambda,\\theta)$ denote the unmarginalized joint density of $X,\\Lambda$. Then The associated likelihood is \n",
    "\n",
    "$$L(\\theta;X,\\Lambda) = \\frac{f(X,\\Lambda,\\theta)}{a(\\theta)}$$ where $a(\\theta)$ is the normalizing constant $\\int\\int f(X,\\Lambda,\\theta) d\\Lambda dX$.\n",
    "\n",
    "Let $\\theta_0$ be a possible value of $\\theta$. \n",
    "\n",
    "$E_{\\theta_0}\\bk{\\frac{f(X,\\Lambda,\\theta)}{f(X,\\Lambda,\\theta_0)}} = \\frac{a(\\theta)}{a(\\theta_0)}$.\n",
    "\n",
    "Also, $E_{\\theta_0}\\bk{\\frac{f(X,\\Lambda,\\theta)}{f(X,\\Lambda,\\theta_0)} \\mid X} = \\frac{a(\\theta|X)}{a(\\theta_0|X)}$, where $a(\\theta|X) = \\int f(x,\\Lambda,\\theta) d\\Lambda$.\n",
    "\n",
    "So, \n",
    "\n",
    "$$ L(\\theta,x) = \\frac{\\int f(X,\\Lambda,\\theta) d\\Lambda}{\\int\\int f(X,\\Lambda,\\theta) d\\Lambda dX} = \\frac{a(\\theta|X)}{a(\\theta)}$$\n",
    "\n",
    "log-likelihood ratio of $\\theta$ and $\\theta_0$ is given by\n",
    "\n",
    "$$\n",
    "l(\\theta,x) - l(\\theta_0,x) = \\log\\bc{\\frac{a(\\theta|x)}{a(\\theta_0|x)}} - \\log\\bc{\\frac{a(\\theta)}{a(\\theta_0)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $r(x,\\Lambda,\\theta,\\theta_0) = \\frac{f(X,\\Lambda,\\theta)}{f(X,\\Lambda,\\theta_0)}$ \n",
    "\n",
    "$l(\\theta,x) - l(\\theta_0,x)$ is approximated by $\\hat{l}(\\theta,x) - \\hat{l}(\\theta_0,x)$, monte carlo integration.\n",
    "\n",
    "MLE of $\\theta$ can be obtained by maximizing this log likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCMC  Estimation\n",
    "\n",
    "The standard MCMC does not work. However, Metropolis-Adjusted Langevin Algorithm (MALA) works...  Basically the proposal distribution becomes\n",
    "\n",
    "$$\n",
    "q(\\zeta^{(in)}|\\zeta^{(i-1)}) = \\N\\bk{\\zeta^{(i-1)} + \\frac{h^2}{2}\\Sigma \\nabla \\log\\bc{\\pi(\\zeta^{(i-1)}|y)},h^2\\Sigma}\n",
    "$$\n",
    "\n",
    "Each component of $\\theta$ does not have support in $\\mathbb{R}$. Therefore, $s$ is a transformation of $\\theta$ which has support in $\\mathbb{R}^{\\dim(\\theta)}$ where $\\Sigma$ is known as the preconditioning matrix and $h$ as the step size. Basically, you proceed with metropolis step that adjusts the proposal distribution using the Langevin algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preferential Sampling\n",
    "\n",
    "Let $s$ denote an unobserved spatial process on region $A$. $X$ denostes a point process on region $A$ and $Y$ denotes a set of measured values.\n",
    "\n",
    "Typically, we use no distribution assumptions on $X$ when $Y|s,X$ follows some regression model. However, that is not correct and may introduce bias in our inference.\n",
    "\n",
    "First article on preferential sampling is by Diggle (2010), where he introduced the idea.\n",
    "\n",
    "**A1:** $S$ is a stationary GP with mean 0 and variance $\\sigma^2$ and correlation fn $\\rho(u,\\phi) = cor(s(x),s(x-u))$.\n",
    "\n",
    "**A2:** Conditiona on $S$, $X$ is inhomogeneous poisson process with intensity $\\lambda(x)=\\exp(\\alpha+\\beta s(x))$\n",
    "\n",
    "**A3:** Conditional on $S$ and $X$, $y_i \\sim N(\\mu+s(x_i),\\tau^2)$.\n",
    "\n",
    "Based on this model, he developed frequentist estimation technique of all the parameters. (Paper by Pati, Reich, Dunso 2011)\n",
    "\n",
    "### Model \n",
    "\n",
    "$p(s_i) \\propto \\exp(\\zeta(s_i))$\n",
    "\n",
    "$y_i \\mid s_i \\sim \\N(\\eta(s_i) + a\\zeta(s_i), \\sigma^2)$\n",
    "\n",
    "### Inference on Bayesian Preferential Sampling Model\n",
    "\n",
    "The model is given by \n",
    "\n",
    "$y_i | s_i \\sim \\N(\\eta(s_i) + a\\zeta(s_i), \\sigma^2), \\text{ where $p(s_i)$ is the same as before}$\n",
    "\n",
    "let $\\zeta(s) = x(s)'\\beta_\\zeta + \\zeta_\\mu(s)$  \n",
    "let $\\eta(s) = x(s)'\\beta_\\eta + \\eta_\\mu(s)$\n",
    "\n",
    "$x(s)$ are covariates at locations $s$. \n",
    "\n",
    "Assume, $\\beta = a\\beta_\\zeta + \\beta_\\eta$.\n",
    "\n",
    "Then, $\\E\\bk{y_i|s_i} = ...$\n",
    "\n",
    "### Prior:\n",
    "$\\zeta_\\mu(s), \\eta_\\mu(s)$ are assigned zero mean GP, $\\beta$ is assigned a multivariate normal prior. In\n",
    "these GP, there are range and variance parameters. They also have to be assigned prior  distributions. Put a flat prior of $a$. The paper proves that $p(a|y,s)$ is proper.\n",
    "\n",
    "When it comes to computation, full GP becomes cumbersome. Therefore, they worked with kernel convolution.\n",
    "\n",
    "Under kernel convolution, the approximated model becomes \n",
    "\n",
    "$y_i | s_i \\sim N(x(s_i)'\\beta + \\sum_{j=1}^N K_{\\psi_\\eta}(s_i-\\phi_j)u_j + a \\sum_{j=1}^N K_{\\psi_\\zeta} (s_i-\\phi_j)v_j, \\sigma^2)$\n",
    "\n",
    "Also approximate the normalizing constant with $\\Delta \\sum_{j=1}^M \\exp(\\zeta(t_j))$ over a grid of $t_j$ and the area of the small cells created by this grid is $\\Delta$.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "p(s_i) = \\frac{\\exp\\bc{x(s_i)'\\beta_\\zeta + \\sum_{j=1}^H K_{\\psi_\\zeta}(s_i-\\phi_j)v_j}}{\\Delta\\sum_{l=1}^M\\exp\\bc{x(t_l)'\\beta_\\zeta + \\sum_{j=1}^H K_{\\psi_\\zeta}(t_l-\\phi_j)v_j}}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Using MCMC, $\\beta$ and $\\sigma^2$ can be foudn with Gibbs. MH for everything else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Galton Watson Branching Process\n",
    "\n",
    "(Looking at only makes in the family.) Let the family start with $Z_1$ individuals. At time $n$, let the number \n",
    "of males be $Z_n$. Every male will add some male children in the next generation.\n",
    "\n",
    "Let $X_{1,n},...,X_{Z_n,n}$ be the number of male children from individuals $1,...,Z_n$ in the $n$th generation.\n",
    "Clearly the number of males at the $n+1$ generation is $Z_{n+1} = X_{1,n}+...+X_{Z_n,n}$.\n",
    "\n",
    "In the simplest GW setup, they assume $X_i\\iid f$, where $f$ is a discrete distribution.\n",
    "\n",
    "Goal: Compute $P(Z_n=0)$ as $n\\rightarrow \\infty$. The prob of ultimate extinction.\n",
    "\n",
    "Assumptions:\n",
    "\n",
    "1. The family size of individuals of the branching process form a collection of independent random variables.\n",
    "2. All families have the same pmf.\n",
    "\n",
    "Let us define $G(s) = \\E\\bk{S^x} = \\sum_{k=0}^\\infty s^k P(X=k)$. Also, $G_n(s) = \\E\\bk{s^{Z_n}}$\n",
    "\n",
    "Let us start with 1 individual: $Z_0=1$.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "G_{n+1}(s) &= \\E\\bk{S^{Z_{n+1}}} \\\\\n",
    "&=\\E\\bk{\\E\\bk{S^{X_1+...+X_{Z_n}}|Z_n}} \\\\\n",
    "&=\\E\\bk{\\bk{G(s)}^{Z_n}} \\\\\n",
    "&=G_n(G(s))\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "So, $G_{n+1}(s)=G_n(G(s))$\n",
    "\n",
    "Lemma: $\\mu=\\E\\bk{Z_1}$ and $\\sigma^2$ = Var($Z_1$) then $E(Z_n)=\\mu^n$ and Var($Z_n$) = $\\begin{cases}n\\sigma^2 & \\text{if } \\mu = 1\\\\ \\frac{\\sigma^2(\\mu^n-1)}{\\mu-1}\\mu^{n-1} &\\text{ otherwise}\\end{cases}$\n",
    "\n",
    "Pf: $G_n(s) = G(G_{n-1}(s))$ take derivative of both side at $s=1$. And derive result for expectation.\n",
    "Take second derivative on both sides at $s=1$ to derive the result for variance. (See Mickey's notes)\n",
    "\n",
    "Based on these facts, we will try to find $P(Z_n=0)$ as n goes to infinity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example:\n",
    "\n",
    "Suppose each $X$ has a mass function. $P(X=k) = (1-p)p^k$. G(s), the MGF is $q/(1-ps)$, where $q$ is $1-p$.\n",
    "\n",
    "Claim: \n",
    "\n",
    "$$\n",
    "G_n(s) = \\begin{cases}\n",
    "\\frac{n-(n-1)s}{n+1-ns} &\\text{ if } p = q = 1/2 \\\\\n",
    "\\frac{q(p^n-q^n-ps(p^{n-1}-q^{n-1}))}{p^{n+1}-q^{n+1}-ps(p^n-q^n)} &\\text{ if } p \\ne q\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Show by induction.\n",
    "\n",
    "For case $p=q$, (and later case $p \\ne q$), It is true for $n=1$. Suppose true for $n=k$, then for $n=k+1$, show it's true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result for general case:\n",
    "\n",
    "Thm: As $n\\rightarrow 0$, $P(Z_n=0)$ goes to prob of ultimate extinction = $\\eta$. Then $\\eta$ is the smallest non-negative root of the equation $s=G(s)$. Also, $\\eta=1$ if $\\mu<1$ and $\\eta<1$  if $\\mu>1$. If $\\mu=1$, then \n",
    "$\\eta=1$ so long as the family size dist has strictly +ve variance.\n",
    "\n",
    "Pf: See Mickey's notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $E(Z_n) > 1$, $\\eta<1$ that is prob of extinct < 1. $E(Z_n)=\\mu^n$.\n",
    "\n",
    "Does $W_n = \\frac{Z_n}{E\\bk{Z_n}}$ converges to any random variable? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Marked Poisson Process\n",
    "\n",
    "Marked poisson process invovle marks - a set of random variables assiociated with each event such that\n",
    "the data generating mechanism is characterized as a poisson process with additional events.\n",
    "\n",
    "# Example:\n",
    "\n",
    "1. Interested in a dataset that contains info of the locations where tornado happened along with the  amount of property damage.  One can also include the number of people died at each location due to tornado. That can be another mark. One  can use several such marks.\n",
    "\n",
    "2. Suppose ecologists are interested in the distributions of some tree species. They try to find the locations of these trees in a forest. In every location, theymeasure diameter at base height for the tree. In this example marks are diameter at base height.\n",
    "\n",
    "3. Longleaf pine trees: Locations and diameter at base height for longleaf pine trees in 200x200 $m^2$ patch of a forest in Thomas County. They found $N=584$ longleaf pine trees in this area. They used DBH for all trees as the mark. Just to fit the model they excluded all trees with DBH less than 2cm.\n",
    "\n",
    "## Data Structure\n",
    "\n",
    "Each point $x_i, i=1,...,n$ in the observation window $\\mathcal R$ has an associated mark $y_i$ in the space $\\mathcal{M}$.\n",
    "This mark can be univariate /multivariate.\n",
    "The idea is to create another non-homogeneous Poisson process on $\\mathcal{R}\\times\\mathcal{M}$ with intensity \n",
    "$\\phi(x,y)=\\lambda(x)h(y|x)$.\n",
    "\n",
    "Modeling $\\lambda(x)$ is equivalent to modeling $(f(x),\\nu)$ where $f(x) = \\frac{\\lambda(x)}{\\nu}$,\n",
    "and $\\nu=\\int_\\mathcal{R} \\lambda(x) dx$.\n",
    "\n",
    "One can build parametric model for both $f(x)$, $h(y|x)$. One can also build nonparametric model for both\n",
    "of the separately. But the more standard practice is to use.\n",
    "\n",
    "$$\\phi(x,y,G) = \\gamma\\int \\kappa^x(x;\\theta^x) \\kappa^y(y;\\theta^y)dG(\\theta^x,\\theta^y)$$\n",
    "\n",
    "Bayesian modeling proceeds by assuming priors on $G$. The standard prior is DP$(\\alpha,G_0)$.\n",
    "\n",
    "## Example \n",
    "\n",
    "In Xiao et al. (2013), they are interested in a Marked Poisson process on [0,1] with a binary mark $z$ and a continuous mark $y$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compactly Supported Correlation Function\n",
    "\n",
    "1. Computational efficiency in GP prediction and estimation\n",
    "2. Fast simulation\n",
    "3. Appeal among practitioners\n",
    "\n",
    "\n",
    "Statisticians like matern function. Geological scientists use:\n",
    "$\\phi(t) = 1-3t/2+t^3/2$ for $t \\in [0,1]$. and 0 otherwise. (spherical correlation)\n",
    "\n",
    "1. Appeal among practitioners: In meteorology, it is widely accepted that correlation between geopotential heights of different locations should be set to zero if they are few hundred kms apart. (Gaspari & Cohn, 1999)\n",
    "    \n",
    "2. Statistical Emulation: In many statistical problems, data are difficult / costly to come by. In that case, it is a common practice to estimate a stochastic process generating the small data and simulate more data from this estimated stochastic process.\n",
    "\n",
    "If we use compactly supported correlation functions, then $\\Sigma$ is sparse. \n",
    "\n",
    "3. Computationally efficient prediction and interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$\\Phi_d$ is the class of cont. functions $\\phi:[0,\\infty)\\rightarrow R$ which represents correlation fn. of a \n",
    "stationary and isotrpoic random field in $R^d$. iff\n",
    "$\\phi(0)=1$ and $((\\phi(x_i,x_j)))_{i,j=1}^n$  is nonnegative definite for any $x_1,...,x_n$ for all $n\\ge 2$.\n",
    "\n",
    "$\\Phi_{d+1} \\subset \\Phi_d$ and it was shown that any element in $\\Phi_d$ is of the form \n",
    "\n",
    "$$\\phi(t) = \\int_{[0,\\infty)} \\Omega_d(tr)d F(r)$$\n",
    "\n",
    "where $F$ is a prob measure in $[0,\\infty)$.\n",
    "\n",
    "$\\Omega_d(t) = \\Gamma(d/2)(2/t)^{d/2-1}J_{(d-2)/2}(t)$ and $J_{(d-2)/2}(t)$ is a Bessel function of the first \n",
    "kind of order $\\frac{d-2}{2}$. Wenland (1995), Matern corr fn belongs to $\\Phi_d$ for all d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THe spherical correlation only works for $d=1,2,3$. But it's fine because d=2 for geodata.\n",
    "\n",
    "How do we construct compactly supported correlation fn which have certain degrees of differentiability?\n",
    "\n",
    "Also need to monitor when they are defined.\n",
    "\n",
    "Define two operators:\n",
    "- $I\\phi(t) = \\frac{\\int_t^\\infty u\\phi(u) du}{\\int_0^\\infty u\\phi(u) du}$, for $t\\ge0$\n",
    "- $D\\phi(t) = \\phi'(t) / (t\\phi''(0))$ if $t>0$, and 1 if $t=0$.\n",
    "\n",
    "Under mild regularity conditions it can be shown that these two are inverse operators. $D(I\\phi(t)) = \\phi(t)$.\n",
    "\n",
    "Thm: If $\\phi$ is an element of $\\Phi_d$, $d /ge3$ and $u\\phi(u)$ is integrable in $[0,\\infty)$, then $I\\phi$ is \n",
    "an element of $\\Phi_{d-2}$.\n",
    "\n",
    "Thm: If $\\phi$ is an element of $\\Phi_d$ and if $\\phi''(0)$ exists, then $D\\phi$ is an element of $\\Phi_{d+2}$.\n",
    "\n",
    "## Wendland Construction (for correlation fn)\n",
    "\n",
    "$\\phi_{\\nu,0}(t)=(1-t)_+^\\nu$ (which is 0 when $t$ out of [0,1)). $\\phi_{\\nu,0} \\in \\Phi_d$  iff $\\nu>\\frac{d+1}{2}$.  This $\\phi_{\\nu,k}$ is not differentiable at 0.\n",
    "\n",
    "$\\phi_{\\nu,k} = I^k\\phi_{\\nu,0}$, Wenland (1995) showed that $\\phi_{\\nu,k}$ is 2k times differentiable at 0.\n",
    "Also, $\\phi_{\\nu,k}\\in \\Phi_d$ if $\\nu \\ge \\frac{d+1}{2}+k$.\n",
    "\n",
    "See [this](http://www.nrcse.washington.edu/pdf/trs45_corr.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kaufman, Schervish, Nychka (2009) talks about tappering GP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
