{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open in [nbviewer](http://nbviewer.jupyter.org/github/luiarthur/stochastic_AMS263/blob/master/notes/notes1.ipynb)\n",
    "$\n",
    "% Latex definitions\n",
    "% note: Ctrl-shfit-p for shortcuts menu\n",
    "\\newcommand{\\iid}{\\overset{iid}{\\sim}}\n",
    "\\newcommand{\\ind}{\\overset{ind}{\\sim}}\n",
    "\\newcommand{\\p}[1]{\\left(#1\\right)}\n",
    "\\newcommand{\\bk}[1]{\\left[#1\\right]}\n",
    "\\newcommand{\\bc}[1]{ \\left\\{#1\\right\\} }\n",
    "\\newcommand{\\abs}[1]{ \\left|#1\\right| }\n",
    "\\newcommand{\\ceil}[1]{ \\lceil#1\\rceil }\n",
    "\\newcommand{\\norm}[1]{ \\left|\\left|#1\\right|\\right| }\n",
    "\\newcommand{\\E}{ \\text{E} }\n",
    "\\newcommand{\\N}{ \\mathcal N }\n",
    "\\newcommand{\\ds}{ \\displaystyle }\n",
    "\\newcommand{\\R}{ \\mathbb{R} }\n",
    "\\newcommand{\\suml}{ \\sum_{i=1}^n }\n",
    "\\newcommand{\\prodl}{ \\prod_{i=1}^n }\n",
    "\\newcommand{\\overunderset}[3]{\\overset{#1}{\\underset{#2}{#3}}}\n",
    "\\newcommand{\\asym}{\\overset{\\cdot}{\\sim}}\n",
    "\\newcommand{\\given}{\\bigg |}\n",
    "\\newcommand{\\M}{\\mathcal{M}}\n",
    "\\newcommand{\\Mult}{\\text{Mult}}\n",
    "\\newcommand{\\F}{\\mathcal{F}}\n",
    "\\newcommand{\\P}{\\mathcal{P}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Overview\n",
    "- Office hours: Th 3-4pm (or by appointment)\n",
    "\n",
    "# Course Description\n",
    "- definitions and properties of stochastic proc\n",
    "- some important stoc. proc.\n",
    "- discrete / cont. time and space stoc proc\n",
    "- hidden markov models\n",
    "- markes poisson poisson process\n",
    "- more involved study of GP\n",
    "\n",
    "# HW (not collected)\n",
    "- 5 assignments\n",
    "- 5-6 full computational\n",
    "\n",
    "# Grade\n",
    "- 2 quizzes @10%\n",
    "- midterm 30%\n",
    "- selected hw problems 10%\n",
    "- project 40%\n",
    "    - report \n",
    "    - presentation\n",
    "    \n",
    "# Reading References\n",
    "1. Grimmett, G. and Stirzaker, D (2001)\n",
    "2. Insua, D.R., Ruggeri, F. & Wiper, M.P. (2012)\n",
    "3. Karlin, S. & Taylor, H.M. (1974) (Theory taken mostly from this)\n",
    "4. Ross, S.M. (1996)\n",
    "5. Guttorp, P. (1995)\n",
    "6. Zucchini, W. & Macdonald, I.L. (2009)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of Stochastic Process (SP)\n",
    "\n",
    "Consider a probability space ($\\Omega,\\F,\\P$) , where $\\omega$ is the sample space of the experiments. Let there be an index set $T$ and a state space $S$. A stochastic process is a collection \n",
    "\n",
    "$$ X =  \\bc{X(\\omega,t): \\omega\\in\\Omega, t\\in T} $$ \n",
    "\n",
    "such that \n",
    "1. for any $n$ and any set of index points $t_i\\in T, i=1,...,n$, $(X_t,...,X_{t_n})$, is an $n$-dim random variable defined on the prob. space ($\\Omega,\\F,\\P$) and taking values in $S^n=S\\times...\\times S$.\n",
    "2. For any fixed $\\omega \\in \\Omega$, $X_\\omega(\\cdot) = X(\\omega,\\cdot): T\\rightarrow S$ is a function defined on $T$ and taking values in $S$, referenced to as a sample path  of the stoc. proc.\n",
    "\n",
    "**Note that SP are distributions over functions.**\n",
    "\n",
    "Basically, SP can be viewed as either a collection of RV's $\\bc{X_t:t\\in T}$, or a collection of random functions $\\bc{X_\\omega: \\omega \\in \\Omega}$, depending on the nature of $T$ and $S$. (i.e. discrete-discrete, cont-disc, disc-cont, cont-cont).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "$(X_1,...,X_n) \\iid (\\Omega,\\F,\\P)$\n",
    "\n",
    "$X(n,\\omega) = X_n(\\omega)$\n",
    "\n",
    "$X = \\bc{X(n,\\omega): n\\in \\mathcal{N}, \\omega\\in \\Omega}$\n",
    "\n",
    "$X_i \\iid Pois(\\lambda)$, then discrete-time, discrete-space.\n",
    "\n",
    "$X_i \\iid \\N(\\mu,\\sigma^2)$, $T=\\mathcal{N}, S=\\mathcal{R}$ (discrete-time, cont-space (normal rv))\n",
    "\n",
    "# Random Walk\n",
    "This SP is a random walk:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(X_i=1)  &= \\pi \\\\\n",
    "P(X_i=-1) &= 1-\\pi \\\\\n",
    "S(n,\\omega) = S_n(\\omega) &= \\suml X_i(\\omega) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "--- \n",
    "\n",
    "Let $z$ be a RV.\n",
    "\n",
    "$P(Z=1)=P(Z=-1) = 1/2$\n",
    "\n",
    "$X(t,\\omega) = X_t(\\omega) = Z(\\omega) \\sin(t)$, for all $t\\ge 0$ cont-time, cont-space.\n",
    "\n",
    "$T=\\mathbb{R}^+, S = \\bc{\\sin t:t>0} \\cup \\bc{-\\sin t:T>0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any SP, we are interested in the distributional properties of $X_t(\\cdot)$. In this regard, we will define the finite dimensional distribution of a SP by looking at the joint distribution of the random vector \n",
    "$F_{t_1,...,t_n}(x_1,...,x_n) = P(X_{t_1} \\le x_1, ...,  X_{t_n} \\le x_n)$.\n",
    "\n",
    "**Q1: Suppose we know the finite dimensional distribution of a stochastic process, is it possible to specify the SP uniquely?** (No)\n",
    "\n",
    "**Q2: Suppose a finite dimensional distr. is provided and specified for every ($X_{t_1},...,X_{t_n}$) for all $t_1,...,t_n$ and for all $n$, does there exist a SP whose finite dim. distr is the one we specified?** (yes, with additional conditions)\n",
    "\n",
    "***\n",
    "\n",
    "Suppose $\\bc{X_t: t\\in T}$, $\\bc{Y_t: t\\in T}$ are two stochastic processes defined on the same $(\\Omega,\\F,\\P)$. \n",
    "We say that $Y$ is a version of $X$ if for every $t\\in T$, we have \n",
    "\n",
    "$$ P(X_t=Y_t) =P(\\bc{\\omega\\in\\Omega: X_t(\\omega) = Y_t(\\omega)}) =1 $$.\n",
    "\n",
    "We say that $X$ and $Y$ are indistinguishable (stronger) if \n",
    "\n",
    "$$ P(X_t=Y_t, \\forall t\\in T) = P(\\bc{\\omega\\in\\Omega: X_t(\\omega) = Y_t(\\omega), \\forall t\\in T}) = 1$$\n",
    "\n",
    "***\n",
    "\n",
    "$Z \\sim N(0,1)$\n",
    "\n",
    "$X_t = 0$\n",
    "\n",
    "$Y_t = 0$ if $t \\ne \\abs{Z}$, and 1 otherwise.\n",
    "\n",
    "$P(X_t\\ne Y_t) = P(\\abs{Z}=t) = 0$. $P(X_t=Y_t, t\\ge 0) = 0$.\n",
    "\n",
    "$Y$ is a version of $X$ and both $X$ and $Y$ have right-cont. sample paths then $X$ and $Y$ are indisinguishable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Given specification of a finite dim. distribution $F_{t_1,...,t_n}(x_1,...,x_n)$ ** for every $n \\ge 1$, and for any $t_1,...,t_n \\in T$, does it correspond to a finite dim. distribution (fdd) of a SP?\n",
    "\n",
    "## Kolmogorov Consistency Conditions\n",
    "If the finite dim. distr.  $F_{t_1,...,t_n}(x_1,...,x_n)$ satisfies\n",
    "\n",
    "1. $F_{t_1,...,t_n,t_{n+1}}(x_1,...,x_n,x_{n+1}) \\rightarrow F_{t_1,...,t_n}(x_1,...,x_n)$  as $x_{n+1} \\rightarrow \\infty$\n",
    "2. For all $n$, $x=(x_1,....,x_n)$, $t=(t_1,...,t_n)$, any permutation $\\pi = (\\pi(1),...,\\pi(n))$ of $\\bc{1,...,n}$, $F_{\\pi_t}(\\pi_x)=F_t(x)$ where $\\pi_x = (x_{\\pi_1},...,x_{\\pi_n})$ and   $\\pi_t = (t_{\\pi_1},...,t_{\\pi_n})$, \n",
    "\n",
    "then there exists a prob. space ($\\Omega,\\F,\\P$) and a collection $x=\\bc{x_t:t\\in T}$ of random variables defined on ($\\Omega,\\F,\\P$) s.t. $F_t$ is the fdd of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Important thing about a SP \n",
    "- we need to study properties of the sample path.\n",
    "  - for any $\\omega\\in\\Omega$, sample path $X_\\omega(t)$ is a function of the index set T $\\bc{X_\\omega(t):t\\in T}$ for a fixed $\\omega$.\n",
    "- A SP is used to assign prior distributions on an unknown function $\\mu(t)$. Typically, we make an assumpltion of the smoothness (differentiability) of the unknown function. Therefore, it is important to know the smoothness of the SP sample paths which are going to be  used as a prior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SP is cont. if for every $\\omega \\in \\Omega$, $X_w(t)$ as a function of $t$ (ie. the sample path) is cont. \n",
    "And more generally, a SP is $\\alpha$-Holder continuous if \n",
    "$ \\abs{X_\\omega(t) - X_\\omega(s)} \\le c\\abs{t-s}^\\alpha $ for all $t, s\\in T$ and for all $\\omega$ for some $c$.\n",
    "\n",
    "If  $\\alpha=1$, then $X_\\omega(t)$ is Lipschitz function.\n",
    "If  $\\alpha>1 \\Rightarrow (X_\\omega(t)-X_\\omega(s))/(t-s) \\le c\\abs{t-s}^{\\alpha-1}$ ... didn't finish\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean function of a SP\n",
    "$\\mu(t) = \\E\\bk{X_t}$ if $t_i,t_j \\in T$, the autocovariance fn. of a SP is given by \n",
    "$C(t_i,t_j) = Cov(X_{t_i},X_{t_j})$ and the autocorrelation is Corr($X_{t_i},X_{t_j}$).\n",
    "\n",
    "An important property of the covariance function is that it is a non-negative definite function.\n",
    "\n",
    "**non-negative definite Matrix: **If $A_{n\\times n}$ is a non-negative definite matrix, then for any $(z_1,...,z_n)'=z$, $z'Az \\ge 0$.\n",
    "\n",
    "**non-negative definite function: ** $c(\\cdot,\\cdot)$ is a non-negative definite fn, if for any $n\\ge 1$ and $t_1,...,t_n$, $z'Cz \\ge 0$ where $C_{ij} = c(t_i,t_j)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance fn of a SP is a non-negative definite fn. \n",
    "Take $z_1,...,z_n$, $t_1,...,t_n \\in T$. \n",
    "\n",
    "$\\suml\\sum_{j=1}^n z_i z_j Cov(X_{t_i},X_{t_j}) = Var(\\suml z_i X_{t_i})$ which is non-negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SP is a white noise process if Cov($X_{t_i},X_{t_j}$) =0 for all $t_i\\ne t_j$.\n",
    "\n",
    "Also, a SP $X$ is a process with uncorrelated increments if for any  $t_i < t_j < t_k < t_l \\in T$, COV($X_{t_j}-X_{t_i}, X_{t_l}-X_{t_k}$) = 0.\n",
    "\n",
    "Two SP $X,Y$ defined on the same prob. space and with the same index set are uncorrelated if the cross-covariance fn \n",
    "$$C_{xy}(t_i,t_j) = Cov(X_{t_i},X_{t_j})= 0, \\forall t_i,t_j \\in T$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the simplifying assumptions to study SP better. The most important of them is the stationarity assumption. Theory and methods of SP are considerably simplified under the stationarity assumption. \n",
    "\n",
    "## Strong stationarity \n",
    "A SP $X$ is strongly stationary if its finite dimensional distributions are invariant under time shift. That is for any finite $n$, for any $t_0$ and for all $t_1,...,t_n \\in T$, $(x_{t_1},...,x_{t_n})$ and $(x_{t_1+t_0},...,x_{t_n+t_0})$ have the same distribution. \n",
    "\n",
    "\n",
    "## Weak Stationarity\n",
    "A SP $X$ is called weakly stationary if $\\forall t\\in T$, $\\E\\bk{X_t}=\\mu$ and for all $t_i,t_j \\in T$, COV($X_{t_i},X_{t_j}$) = $c(t_i-t_j)$ a fn of $t_i-t_j$ only.\n",
    "\n",
    "\n",
    "SS implies WS, WS not neccessarily imply SS, except in GP.\n",
    "\n",
    "\n",
    "From the Fourier analysis, any fn, $f: R \\rightarrow Q$ with periodicity and continuity has a unique Fourier expansion, $f(x) = a_0/2 + \\sum_{n=1}^\\infty a_n\\cos(nx) + b_n \\sin(nx)$  which expresses $f$ as a sum of ranging proportions of regular  oscillations. In some sense, weak stationarity processes are similar to periodic fn since their autocov fns are invariant under time shifts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral theorem for autocov fn.\n",
    "\n",
    "Consider a cont. time WS SP $X={X_t:t\\in\\mathbb R}$ with a strictly positive variance. If the autocov fn. $r(t)$ of $X$ is cont. at $t=0$, then there exists a distr fn. $F$ s.t. $r(t) = \\int_{-\\infty}^\\infty \\exp(itu) dF(u)$.\n",
    "\n",
    "If $F$ is the distribution fn. for a cont. RV, with density $f$, then inverse fourier transformation tells us that \n",
    "$$f(u) = \\frac{1}{2\\pi} \\int_{-\\infty}^\\infty \\exp(-itu) \\mu(t) dt$$ where $f$ is differentiable at $x$. (this f is known as the spectral density, which is the inverse Fourier transform of $\\mu$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $T = \\mathbb Z$, $X = \\bc{X_n: n\\in \\mathbb Z}$. \n",
    "\n",
    "\\begin{split}\n",
    "r(n) &= \\int_{-\\infty}^\\infty \\exp(inu) dF(u) \\\\\n",
    "     &= \\int_{-\\infty}^\\infty \\exp(in(u+2\\pi s)) dF(u), \\text{ where $s$ is any integer} \\\\\n",
    "\\end{split}\n",
    "\n",
    "So, $r(u) = \\int_{-\\pi}^\\pi\\exp(inu) dF(u)$. And spectral density comes from the inverse Fourier trans of $r(u)$. Hence, $f(u) = \\frac{1}{2\\pi} \\sum_{n=-\\infty}^\\infty \\exp(-inu)r(n)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of convergence of seq. of r.v. converge to a r.v X, \n",
    "1. convergence in probability:\n",
    "2. convergence in distribution:\n",
    "3. Mean-squared conv.:\n",
    "4. Almost-sure conv.: $X_n(\\omega)\\rightarrow X(\\omega), \\forall \\omega\\in\\Omega$\n",
    "    \n",
    "# Weak Law of Large Numbers (WLLN)\n",
    "$X1,...$ are iid rv with $\\E\\bk{X_i}=\\mu$ and Var($X_i=\\sigma^2<\\infty$) then $\\frac{1}{n}\\suml X_i \\rightarrow \\mu$ in probability.\n",
    "\n",
    "# Kolmogorov LLN\n",
    "$X_1,...$ are independent RV with $\\E\\bk{X_i}=0$ and $\\E\\bk{X_i^2} < \\infty$, $\\sum_i^\\infty \\frac{1}{i^2}\\E\\bk{X_i^2} < \\infty$ then $\\frac{1}{n}\\suml X_i \\rightarrow 0$ almost surely.\n",
    "\n",
    "# Ergodic Thm. for Weakly Stationary SP\n",
    "If $X = \\bc{X_j:j\\ge 1}$ is a WS SP, then there exists a random variable $Y$ such that $\\E\\bk{Y}=\\E\\bk{X_1}$ and\n",
    "$\\frac{1}{n}\\suml X_i \\rightarrow Y$ in mean square."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of important SPs\n",
    "\n",
    "## Example 1 (Brownian Motion)\n",
    "Suppose there is a collection of RV's $\\bc{B_s: s\\ge 0}$, having a number of prroperties as following:\n",
    "1. $B_0=0$ (convention, the origin)\n",
    "2. $0\\le s<t<\\infty$,  $B_t-B_s\\sim \\N(0,t-s)$.\n",
    "3. $0\\le s<t<\\infty$,  $B_t-B_s$ is independent of $B_s$.\n",
    "4. The function $t\\rightarrow B_t$ is a continuous function.\n",
    "\n",
    "Condition 3. can be replaced by the following condition:\n",
    "- given $t_1 < t_2 < t_3 < t_4$, $B_{t_4}-B_{t_3}$ is independent of $B_{t_2}-B_{t_1}$.\n",
    "\n",
    "\n",
    "Einstein showed that the Brownian motion is the solution to the heat equation.\n",
    "\n",
    "It can be shown that the following is true (by solving $\\E\\bk{W_s-W_t}=0$ and $Var(W_s-W_t)=t-s$):\n",
    "\n",
    "Let $t_1,...,t_n\\in\\mathbb{R}$ , for any $n$,\n",
    "\\begin{split}\n",
    "\\begin{pmatrix} B_{t_1}\\\\ ... \\\\ B_{t_n} \\\\\\end{pmatrix} \\sim \\N(0,\\Sigma), \\text{ with } \\Sigma_{i,j}=\\min\\bc{t_i,t_j}\n",
    "\\end{split}\n",
    "\n",
    "So, any finite dimensional distribution is Gaussian. Note that it is easy to get the distribution of any linear combo given the multivariate distribution.\n",
    "\n",
    "Brownian motion is a special case of the GP. The sample paths for a Brownian motion are not differentiable, Einstein proved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2\n",
    "\n",
    "A GP is a SP for which any finite dimensional distribution is Normal, i.e. $\\bc{X_t:t\\in\\mathbb{R}}$ is a GP if \n",
    "for any $t_1,...,t_n$, $n\\in \\mathbb N$, \n",
    "\\begin{pmatrix} X_{t_1}\\\\ ... \\\\ X_{t_n} \\\\\\end{pmatrix} follorws a multivariate Normal density.\n",
    "\n",
    "## WEakLy Stationary GP\n",
    "\n",
    "A WSGP is determined by its mean and covariance kernel. Kernel is $c(t)=cov(x_s,x_{s+t})$. Therefore, the finite dim. distr. of a GP with a specified cov. kernel $c(t)$ is given by \n",
    "\\begin{split}\n",
    "\\begin{pmatrix} X_{t_1}\\\\ ... \\\\ X_{t_n} \\\\\\end{pmatrix} \\sim \\N(\\mu,\\Sigma), \\text{ with } \\Sigma_{i,j}=c(t_i-t_j)\n",
    "\\end{split}\n",
    "\n",
    "Common covariance functions\n",
    "1. modified Bessel function of the 2nd kind\n",
    "    - has 3 params, $\\nu,\\phi,\\sigma^2$. \n",
    "    - note that $c(0)=\\sigma^2$. so $\\sigma^2 = var(X_t)$.\n",
    "    - cov($x_s,x_{s+t}$) increases if $\\phi$ decreases. So $\\phi$ controls the correlation of the SP.\n",
    "    - $\\nu$ controls the smoothness of the SP. (aka the differentiability) In fact, Michael Stein showed that the sample paths of a GP with the above kernel is $\\ceil{\\nu-1}$ times differentiable.\n",
    "    \n",
    "Cases for $\\nu$\n",
    "1. If $\\nu=1/2$, $c(t)=\\sigma^2\\exp(-\\phi t)$: exponential corr. fn (statisticians use this)\n",
    "2. If $\\nu=\\infty$, $c(t)=\\sigma^2\\exp(-\\phi t^2)$: Gaussian corr. fn (machine learners use this because it produces better predictions, but the cov. matrices are not invertable, so they don't use it.)\n",
    "3. If $\\nu=3/2$, $c(t)=$ nasty: once differentiable.\n",
    "\n",
    "Sample paths are nowhere differentiable  in 1. In 2, sample paths are infintely differentiable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- splines: [David Rupert Semi-parametric modeling]( https://books.google.com/books/about/Semiparametric_Regression.html?id=Y4uEvXFP2voC)\n",
    "    - splines with time series for LLNL data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# GP Regression\n",
    "\n",
    "$(y_1,x_1),...,(y_n,x_n)$\n",
    "\n",
    "$y_i = f(x_i) + \\epsilon_i, \\text{ where} \\epsilon_i \\sim N(0,\\sigma^2)$\n",
    "\n",
    "$f$ is an unknown fn.\n",
    "\n",
    "Goal: Prior distribution on $f$. \n",
    "\n",
    "$f \\sim GP(\\mu,~\\kappa(\\cdot,\\cdot,\\phi))$\n",
    "\n",
    "For any $x_i,x_j$, apriori, cov($f(x_i),f(x_j)$) = $\\kappa(x_i,x_j,\\phi,\\tau^2)$, and $\\E\\bk{f(x_i)} = \\mu$\n",
    "\n",
    "For the sake of analysis, lets assume $\\kappa(x_i,x_j,\\phi,\\tau^2) = \\tau^2\\exp(-\\phi\\abs{x_i-x_j})$\n",
    "\n",
    "Refer to [my website](http://luiarthur.github.io/ucsc_notes/advBayesComputing/17/) for notes on GP regression.\n",
    "\n",
    "Note that the full condition for $\\theta = f(x) \\sim N((I_n/\\sigma^2 + 1/\\tau^2 H(\\phi)^{-1})^{-1}(y/\\sigma^2 + 1/\\tau^2 \\mu H(\\phi)^{-1} 1_n), (I_n/\\sigma^2 + 1/\\tau^2 H(\\phi)^{-1})^{-1})$. \n",
    "\n",
    "$\\phi$ gets a uniform prior\n",
    "$\\tau^2, \\sigma^2$ get inverse-gamma distribution\n",
    "\n",
    "$\\tau^2 H(\\phi) = $ covariance function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Convolution\n",
    "\n",
    "$f(x)$ is a GP $f(x) = \\int \\kappa(x,u) z(u)du$ where $z(u)$ is a white noise SP. i.e. $z(u_1) and z(u_2)$ are independent for any $u_1,u_2$ and $z(u_i) \\sim \\N(0,\\sigma^2)$.\n",
    "\n",
    "Trade-off: The convolution process overestimates variability in $f$ and calls it noise. In otherwords, it smooths over (smaller) local trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse GP (ML 2007) Predictive Process (Stats 2008)\n",
    "\n",
    "$y_i = \\E\\bk{f(x_i) | f(u_1),...,f(u_m)} + \\epsilon_i$\n",
    "\n",
    "where $u_1,...,u_m$ are some specified points. They are typically user defined.\n",
    "\n",
    "$f \\sim GP(0,\\kappa)$\n",
    "\n",
    "$\\E\\bk{f(x_i) | f(u_1),...,f(u_n)} = $ the typical conditional expectation of MVN.\n",
    "\n",
    "So, $y_i  = (\\kappa(x_i,u_1),...,\\kappa(x_i,u_1)) K^{-1}f + \\epsilon_i$\n",
    "\n",
    "So, $y  =  C~ K^{-1}f + \\epsilon$, where $\\epsilon \\sim\\N(0,\\sigma^2 I_n)$\n",
    "\n",
    "- GP: $O(n^3)$\n",
    "- Predictive process: $O(nm^2)$\n",
    "    - choose $m = \\sqrt{n}$\n",
    "    - So, for pp $\\approx O(n^2)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chains\n",
    "\n",
    "Assume $X$ is a SP with $T=\\bc{0,1,...}$ and $S$ also finite or (countably infinite).\n",
    "(i.e. $X = \\bc{X_0,X_1,...}$. And $X_i \\in S$, where $S$ is countable.)\n",
    "This SP is a Markov Chain if $P(X_n=s | X_0=x_0,...,X_{n-1}=x_{n-1}) = P(X_n=s | X_{n-1}=x_{n-1})$  for all\n",
    "$n\\ge 1$ and for all $s,x_0,x_1,...,x_{n-1} \\in S$.\n",
    "\n",
    "## Galton-Watson Branching Process\n",
    "\n",
    "Let $X_0$ be the number of people with Victorian surnames. $X_m$ are the number of people with Victorian surnames at generation $m$. $X_{m+1} = \\zeta_1 + ... + \\zeta_{x_m}$, where $\\zeta_i$ is the number of males produced by\n",
    "the $i^{th}$ Victorian person at the $m^{th}$ generation.\n",
    "\n",
    "To know the distribution of $X_{n+1}$, it suffices to know only $X_n$.\n",
    "\n",
    "# Homogeneous SP\n",
    "\n",
    "A SP $X$ is homogeneous if $P(X_{n+1}=j|X_n=i) = P(X_1=j|X_0=i)$. If the Markov Chain is homogeneous and the state\n",
    "space $S$ is finite, wlog, $s=\\bc{1,...,m}$. In this case ,the Markov chain can be fully specified by an $m \\times m$ matrix P, known as the transition matrix, where $P_{i,j} = P(X_{n+1}=j|X_n=i)$.\n",
    "\n",
    "Theorem: \n",
    "- $p_{i,j} \\ge 0$.\n",
    "- $\\ds\\sum_{j=1}^m p_{i,j} =1, \\forall i$ (because it's going to one of all possible states)\n",
    "- $P(X_{n+m}=j | X_m=i) = \\sum_k P(X_{n+m}=j,X_{m+r}=k|X_m=i)$, where $1\\le r \\le n-1$\n",
    "  - $= \\sum_k P(X_{n+m}=j|X_m=i, X_{m+r}=k)P(X_{m+r}=k|X_m=i)$\n",
    "- $P(X_{m+k}=j|X_m=i)$ is the $(i,j)-th$ element of $P^k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapman-Kolmogorov Equation\n",
    "\n",
    "define $P(X_{n+m}|X_{m+r} = k) = p_{kj}(m+r,m+n)$.\n",
    "\n",
    "Then, $p_{ij}(m,m+n) = \\sum_k p_{kj}(m+r) p_{ik}(m,m+r)$\n",
    "\n",
    "In Markov Chain, $i,j$ element of $p^n = \\sum_k\\bc{P^{n-k}_{k,j}}\\bc{P^k_{i,k}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Markov Chains (MC)\n",
    "\n",
    "Markov chain, $X=\\bc{X_0,X_1,...}$ where each $X_i$ can take only ffinite number of values. $X$ is a Markov chain if $X_{n+1} | X_n,X_{n-1},...,X_0 \\overset{L}{=} X_{n+1}|X_n$.\n",
    "\n",
    "We are interested in time homogeneous MC. \n",
    "$$P(X_{n+1}|X_n=i) = P(X_1=j|X_0=i)$$\n",
    "\n",
    "Thus, we can represent a finite state MC by a transition matrix $P$ st $p_{ij}=P(X_{n+1}=j|X_n=i), \\forall n=1,...$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Ehrenfest Diffusion Model\n",
    "\n",
    "Let there be two adjacent containers. The two containers have $2a$ balls in total. suppose a ball is randomly selected out of the $2a$ balls and is moved to a different container. \n",
    "\n",
    "$X_n$ = the number of balls in container A at time $n$\n",
    "\n",
    "$P(X_{n+1}=a+j|X_n=a+i) = p_{ij}$, where $p_{ij} = \\frac{a-i}{2a}$ if $j=i+1$, $\\frac{a+i}{2a}$ if $j=i-1$, 0 otherwise.\n",
    "\n",
    "Transition matrix is given by $P$ whose i,j -th element is $p_{ij}$. State space $S=\\bc{0,...,2a}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Markov chains,\n",
    "\n",
    "A state $i$ is called **persistent** (or recurrent) if $P(X_n=i \\text{ for some }n\\ge 1 | X_0=i)=1$. (Discrete state-space)\n",
    "\n",
    "If the prob $P(X_n=i \\text{ for some }n\\ge 1 | X_0=i)<1$ then the state is called a **transient** state.\n",
    "\n",
    "Define $f_{ij}(n)=P(X_1\\ne j,X_2\\ne j,...,X_{n-1}\\ne j, X_n=j|X_0=i)$.\n",
    "\n",
    "Let $f_{ij}=\\sum_{n=1}^\\infty f_{ij}(n)$ = prob that it goes from $i$ to $j$ at some time.\n",
    "\n",
    "$j$ is persistent if $f_{jj}=1$.\n",
    "\n",
    "Is there an easier way to check if the state $j$ is persistent?\n",
    "\n",
    "Yes, define $P_{ij}(s) = \\sum_{n=0}^\\infty s^n p_{ij}(n)$, $F_{ij}(s) =\\sum_{n=0}^\\infty s^n f_{ij}(n)$.\n",
    "\n",
    "Here $p_{ij}(n) = P(X_n=j|X_0=i)$.\n",
    "\n",
    "By convention, take $p_{ij}(0) = 1$ if $i=j$ and 0 o.w. And $f_{ij}(0)=0$ for all i,j. \n",
    "$f_{ij} = \\sum_{n=1}^\\infty f_{ij}(n) = F_{ij}(1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theorem\n",
    "\n",
    "- a) $P_{ii}(s) = 1 + F_{ii}(s) P_{ii}(s)$\n",
    "- b) $P_{ij}(s) = F_{ij}(s) P_{jj}(s)$ for $i\\ne j$. \n",
    "\n",
    "### Corollary\n",
    "\n",
    "- a) State $j$ is persistent if $\\sum_{n=1}^\\infty p_{jj}(n)=\\infty$ $\\iff$ $\\sum_n p_{ij}(n)=\\infty$  st $f_{ij}>0$.\n",
    "  - in other words, the chain should move from state $j$ to state $j$ infinitely often.\n",
    "- b) State $j$ is transient if $\\sum_n p_{jj}(n) < \\infty \\iff \\sum_n p_{ij}(n)<\\infty$, for all $i$.\n",
    "- c) State $j$ is transient then $p_{ij}(n)\\rightarrow 0$ for all $i$.\n",
    "\n",
    "Let $N(i)$ be the number of times a chain visits its starting point $i$.\n",
    "\n",
    "\\begin{split}\n",
    "P(N(i) = \\infty) = \n",
    "    \\begin{cases}\n",
    "    1,  \\text{ if $i$ is persistent} \\\\\n",
    "    0   \\text{ ow}\n",
    "    \\end{cases}\n",
    "\\end{split}\n",
    "\n",
    "$T_i = \\min\\bc{n\\ge 1, X_n=i}$. Clearly,  $P(T_i=n) = f_{ii}(n)$.\n",
    "\n",
    "So, $P(T_i=\\infty | X_0=i ) > 0$ iff $i$ is transient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **mean recurrent** time $\\mu_i$  of state $i$ is \n",
    "$\\E\\bk{T_i|X_0=i} = \\begin{cases} \n",
    "\\sum_n n f_{ii}(n), \\text{ if $i$ is persistent} \\\\\n",
    "\\infty, \\text{ ow}\n",
    "\\end{cases}$.\n",
    "\n",
    "If $i$ is a persistent state and $\\mu_i=\\infty$ then $i$ is known as the null state. If $\\mu_i < \\infty$ then \n",
    "$i$ is called a non-null state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $d(i) = gcd\\bc{n:p_{ii}(n)>0}$. If $d(i)=1$ then state $i$ is called **aperiodic**, o.w. it is called periodic.\n",
    "\n",
    "A state is called **ergodic** if it is persistent, non-null, and aperiodic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call state $i$ communicate with state $j$, denoted by $i\\rightarrow j$, if $p_{ij}(m)>0$ for some $m$,\n",
    "and $i,j$ intercommunicate if $i\\rightarrow j$ and $j\\rightarrow i$. We denote it by $i\\leftrightarrow j$.\n",
    "\n",
    "Thm: If $i \\leftrightarrow j$ then:\n",
    "- a) $i,j$ have the same period\n",
    "- b) $i$ is transient iff $j$ is transient\n",
    "- c) $i$ is null persistent iff $j$ is null persistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A set of states $C$ is called **irreducible** if $i\\leftrightarrow j$ for all $i,j \\in C$.\n",
    "\n",
    "Lemma: If $S$ is a finite state space then at least one state is persistent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sterling's approximation: $n! \\approx n^{n+1/2}e^{-n}\\sqrt{2\\pi}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gambler's Ruin Problem\n",
    "\n",
    "You and your friend start with $2N$ dollars. At each time, you gamble and win with probability $p$. If you win you receive \\$1. If you lose, you lose \\$1.\n",
    "\n",
    "Q: If you start with \\$i, what is the probability that you will lose all your money before your friend?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stationary Distribution\n",
    "\n",
    "$\\mathbf \\pi$ is called a stationary distribution of a MC if $\\mathbf \\pi = \\bc{\\pi_j: j\\in S}$ \n",
    "\n",
    "- a) $\\pi_j \\ge 0$ for all $j$, and the sum of the $\\pi_j$'s is 1\n",
    "- b) $\\pi = \\pi P$, where $P$ is the transition matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "An irreducible chain has a stationary distribution $\\pi$ iff all states are non-null persistent. In this case, $\\pi$ is the unique stationary distribution given by $\\pi_i=\\mu_i^{-1}$ where $\\mu_i$ is the mean recurrence time.\n",
    "\n",
    "The class of states in a Markov Chain can be divided into a number of sets $\\bc{s_1,...,s_p}$ s.t. $s=s_1 \\cup s_2 \\cup ... \\cup s_p$. One of these $s_i$'s consists of all transient states and the rest are all irreducible states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Walk with reflecting barrier of 0\n",
    "\n",
    "$X_n$ is a random walk $X_n \\in \\bc{0,1,2,...}$ if $p_{00}=q$, $p_{i,i+1}=p$ for all $i\\ge 0$, $p_{i,i-1}=q$ for $i\\ge 1$. $p+q=1$.\n",
    "\n",
    "To find the stationary distribution we need to solve the equation: \n",
    "\n",
    "$$\\pi = \\pi P$$\n",
    "\n",
    "So, $\\pi_1 = \\pi_0 p/q$, etc. And $\\pi_n = \\pi_0 (p/q)^n$.\n",
    "And $\\pi_0 + \\sum_{n=1}^\\infty \\pi_n = 1$. So $\\pi_0 \\p{1 + \\sum_{n=1}^\\infty (p/q)^n} = 1$.\n",
    "\n",
    "In order to have $\\pi_0 < \\infty$ we have have $\\sum_{n=1}^\\infty (p/q)^n <\\infty$ iff $p<q$.\n",
    "\n",
    "Therefore, $p<q$ iff the stationary distribution exists. This is an irreducible MC, which implies that all states in this MC are non-null persistent iff $p<q$.\n",
    "\n",
    "If $p<q$ then what is the mean recurrence time?\n",
    "\n",
    "if $p<q$, $\\pi_i = (p/q)^i \\pi_0 = (p/q)^i (1-p/q)$. So the mean recurrence time is $\\mu_i = 1/\\pi_i$ = etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "Say, you have taken a lot of courses. Every Monday, you receive 2 new assignments with prob 2/3 and 3 new assignments with prob 1/3. Every week, b/w Monday morning and Friday afternoon, you finish 2 assignments. (They might be new ones or unfinished from previous weeks.) If you have any unfinished aassignments on Friday afternoon, you finish only one of them by Monday morning with probability $c$ and don't finish even 1 with probability $1-c$.\n",
    "\n",
    "If this quarter goes forever, how many weeks is it before you can expect a weekend with no homework?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let Xn be the number of unfinished hw at the end of the nth Friday after the quarter starts.\n",
    "\n",
    "$X_n \\in \\bc{0,1,...}$\n",
    "\n",
    "$X_0 = 0$\n",
    "\n",
    "$P(X_1=0|X_0=0)=2/3$\n",
    "\n",
    "$P(X_1=1|X_0=0) = 1/3$\n",
    "\n",
    "In general, $P(X_n=i-1|X_{n-1}=1) = 2c/3$.\n",
    "\n",
    "$P(X_n=i+1|X_{n-1}=i) = (1-c) /3$, $i >0$\n",
    "\n",
    "$P(X_n=i|X_{n-1}=i) = (2-c) /3$\n",
    "\n",
    "Then, make a transition prob matrix. Solve $\\mu_0 = 1/\\pi_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_done <- function(p_do_1_hw_in_wkend, limit=10000) { # do no hw wp 1-c\n",
    "    p_2_new_hw <- 2/3 # p 3 new hw = 1/3\n",
    "    \n",
    "    num_hw <- 0\n",
    "    i <- 0\n",
    "    while((num_hw > 0 || i==0) && (i < limit)) {\n",
    "       num_hw <- num_hw + ifelse(p_2_new_hw>runif(1), 2, 3) -2 -ifelse(p_do_1_hw_in_wkend>runif(1), 1, 0)\n",
    "        i <- i+1\n",
    "    }\n",
    "    \n",
    "   return(i)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#x <- sapply(1:1000, function(x) mean_done(.4,limit=1000))\n",
    "#c(mean(x),sd(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thm: For an irreducible aperiodic chain, we have $lim_{n\\rightarrow\\infty} p_{ij}(n)=1/\\mu_j$ for all i,j.\n",
    "\n",
    "Corollaries: \n",
    "1. if a chain is transient then $\\mu_j=\\infty$ implies $\\lim_{n\\rightarrow\\infty} p_{ij}(n)=0$ for all i,j\n",
    "2. If the chain is non-null persistent, $lim_{n\\rightarrow\\infty} p_{ij}(n)=\\pi_j$ for all i,j.\n",
    "3. Limit does not depend on the state where the chain started.\n",
    "4. If $bc{X_n}$ is an irreducible chain with period $d$, then $y_n = X_{nd}$ is an aperiodic chain, which implies $lim_{n\\rightarrow\\infty}P(Y_n=j|Y_0=j)\\rightarrow d/\\mu_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reversibility of a MC\n",
    "\n",
    "Suppose $\\bc{X_n: 0 \\le n \\le N}$  is an irreducible nun-null MC with a transient matrix $P$ and a stationary distr $\\pi$. Suppose $X_n$ has marginal distr $\\pi$. Define $Y_n=X_{N-n}$, with $0\\le n\\le N$.\n",
    "\n",
    "Thm: $P(Y_{n+1}=j|Y_n=i) = \\pi_j/\\pi_i p_{ji}$. \n",
    "\n",
    "A chain is called reversible if $P(Y_{n+1}=j|Y_n=i) = P(X_{n+1}=j|X_n=i)$ iff $\\pi_j/\\pi_i p_{ji}=p_{ij}$ for all $i,j$.\n",
    "\n",
    "Thm: Suppose a distr $\\pi$ exists st $\\pi_j p_{ji} = \\pi_i i{_ij}$ for all $i,j$. Then $\\pi$ is the stationary distr. of the chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a continuous chain, we are interested in a transition kernel $T(x,y)$  s.t. the function $x\\rightarrow T(x,B)$ is measureable for $B\\in\\mathcal{B}$, where $B\\rightarrow T(x,B)$ is a prob measure. \n",
    "\n",
    "Clearly, a staionary distr $\\pi(x)$ is given by a density which solves $\\int T(x,y) \\pi(x)dx = \\pi(y)$\n",
    "\n",
    "Similar to $\\pi_i =\\sum_{j\\in S} \\pi_j p_{ji}$\n",
    "\n",
    "Refer to hand-written notes and look up what is gibbs sampler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Metropolis Proof\n",
    "\n",
    "See hand-written notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous time Markov Chain\n",
    "\n",
    "Until now, we have focused on $\\bc{X_n: n=0,1,...}$. Now we will focus on $\\bc{X(t): t\\ge0}$. \n",
    "\n",
    "$\\bc{X_t\\in S}$. $S$ can be (1) discrete or (2) continuous.\n",
    "\n",
    "## Example: \n",
    "\n",
    "No. of accidents up to time $t = X(t)$ then $t$ is a continuous indexing set, but $X(t) \\in \\mathbb{Z}^+$.\n",
    "\n",
    "\n",
    "## Poisson Process\n",
    "\n",
    "A **poisson process** with intensity $\\lambda$ is  a process $N=\\bc{N(t):t\\ge0}$ taking values in $S=\\bc{0,1,...}$ s.t. \n",
    "\n",
    "1. $N(0) = 0$ if  $s<t$ then $N(s) \\le N(t)$\n",
    "2. $P(N(t+h) = n+m|N(t)=n)= \\begin{cases}\\lambda h+o(h) & \\text{if $m=1$}\\\\ o(h) & \\text{if $m>1$} \\\\ 1-\\lambda h + o(h) & \\text {if } m=0\\end{cases}$\n",
    "3. if $s<t$ the number $N(t)-N(s)$ is independent of $N(s)$.\n",
    "\n",
    "Remark: $o(h)/h \\rightarrow 0$ as $h\\rightarrow 0$. And $o(h)$ is any function (for small $h$) s.t. this will hold.\n",
    "\n",
    "**Thm:** $N(t)$ is the number of events up to time $t$ $\\sim $ Pois($\\lambda t$). (Search for the proof.)\n",
    "\n",
    "Notation is used here... $p_j'(t) = \\lambda p_{j-1}(t) - \\lambda p_j(t)$ for $j=1,...$.\n",
    "\n",
    "It can be shown, $p_0'(t) = -\\lambda p_0(t)$. So (diff eq) $p_0(t) = e^{-\\lambda t}$.\n",
    "\n",
    "So $p_1'(t) = \\lambda p_0(t) - \\lambda p_1(t)$, which is a first order linear diff eq. So $p_1(t) = e^{-\\lambda t}\\lambda t$. Using recursion, $p_n(t) = e^{-\\lambda t} \\frac{(\\lambda t)^n}{n!}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interarrival Times\n",
    "\n",
    "$T_0 = 0$, $T_n=\\inf\\bc{t:N(t) = n}$\n",
    "\n",
    "Define $X_n = T_n -T_{n-1}$ are the interarrival times, in fact, $T_n = \\suml X_i$, $N(t) = \\max\\bc{n: T_n\\le t}$.\n",
    "\n",
    "**Thm:** $X_1,.X_2,... \\iid \\text{Exp}(\\lambda)$. (Search for proof)\n",
    "\n",
    "One can start with a process which has interarrival times iid Exp($\\lambda$). One can show the process has to be a Poisson process with intensity $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Birth Process\n",
    "\n",
    "A **birth process** with intensities $\\lambda_0,.\\lambda_1,...$ is a process $\\bc{N(t): t\\ge 0}$  taking values in $S=\\bc{0,1,2,...}$ st \n",
    "\n",
    "1. $N(0) = 0$ if  $s<t$ then $N(s) \\le N(t)$\n",
    "2. $P(N(t+h) = n+m|N(t)=n)= \\begin{cases}\\lambda_n h+o(h) & \\text{if $m=1$}\\\\ o(h) & \\text{if $m>1$} \\\\ 1-\\lambda_n h + o(h) & \\text {if } m=0\\end{cases}$\n",
    "3. if $s<t$ then, conditional on $N(s)$, $N(t)-N(s)$ is independent of  all arrivals prior to $s$.\n",
    "\n",
    "Clearly, $\\lambda_n =\\lambda$ for all $n$ implies poisson process.\n",
    "\n",
    "$\\lambda_n = n\\lambda$ for all $n$ implies simple birth process.\n",
    "\n",
    "$\\lambda_n = n\\lambda + \\nu$ for all $n$ implies simple birth process with immigration.\n",
    "\n",
    "Using the same technique as in Poisson process, one can show that $p_0'(t) = -\\lambda_0 p_0(t)$, and \n",
    "$p_n'(t) = -\\lambda_n p_n(t) + \\lambda_{n-1} p_{n-1}(t)$ for all $n\\ge 1$, with boundary conditions $p_0(0)=1, p_n(0)=0$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For a simple birth process, differential equations lead to a closed form interesting solution.\n",
    "\n",
    "$p_n'(t) = -\\lambda n p_n(t) + \\lambda (n-1) p_{n-1}(t)$ for simple birth-death process.\n",
    "\n",
    "Boundary Conditions: $p_1(0) = 1, p_n(0)=0$. \n",
    "\n",
    "$p_n(t) = P(X(t)=n)$. Thus $p_1(0) = 1 \\Rightarrow P(X(0)=1)=1$.\n",
    "\n",
    "Solve this:\n",
    "\n",
    "$p_n'(t) +\\lambda n p_n(t) = \\lambda (n-1) p_{n-1}(t)$\n",
    "\n",
    "$p_1(t) = e^{-\\lambda t}$\n",
    "$p_2(t) = e^{-\\lambda t - 1}$\n",
    "\n",
    "In general, $p_n(t) = e^{-\\lambda t} (1-e^{-\\lambda t})^{n-1}$ for $n\\ge 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the birth process starts with 1 individual. What if it starts with $N$ individuals?\n",
    "\n",
    "A Yule  process that starts with $N$ individiauls can be conceptualized as sum of $N$ independent Yule processes each starting with only 1 individual.\n",
    "\n",
    "$X(t)$ = process that starts with $N$ individuals.\n",
    "\n",
    "$X(t) = X_1(t) + ... + X_N(t)$.\n",
    "\n",
    "$\\E\\bk{s^{X(t)}} = \\E\\bk{s^{X_1(t) + ... X_N(t)}} = \\p{f(s)}^N$. Now we know $f(s) = \\sum_1^\\infty p_n(t) s^n = e^{-\\lambda t} s \\sum_{n=1} ^\\infty\\bk{(1-e^{-\\lambda t})^s}^{n-1} = \\frac{se^{-\\lambda t}}{1-(1-e^{-\\lambda t})s}$\n",
    "\n",
    "\n",
    "$\\E\\bk{s^{X(t)}} = \\bk{\\frac{s e^{-\\lambda t}}{1- (1-e^{-\\lambda t})s}}^N$\n",
    "\n",
    "**Binomial Series:** $(1-x)^{-N} = \\sum_{m=0}^\\infty{m+N-1\\choose m} x^m$, where $x \\in (0,1)$.\n",
    "\n",
    "Using the binomial series, we can show that $p_{Nn}(t) = P(X(t)=n | X(0) = N) = {n-1\\choose n-N}\\p{e^{-\\lambda t}}^N \\p{1-e^{-\\lambda t}}^{n-N}$, $n \\ge N$.\n",
    "\n",
    "**Check out [theorem 1.5](http://www.columbia.edu/~ks20/stochastic-I/stochastic-I-PP.pdf) interesting theorem regarding the conditional distribution of the arrival times given and prior to the time event $n$ occurs of a poisson process. i.e. $T_1,...T_{n-1} | N(t)=n $ **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Birth death process (look up in book)\n",
    "\n",
    "### Simple birth and death with immigration process (most famous)\n",
    "\n",
    "Used heavily in biology to understand growth of species. This is  a b-d process with $\\lambda_n = \\lambda n+a$ and $\\mu_n = n\\mu$,  \n",
    "where $a$ is the rate of immigration, $\\lambda$ is the rate of birth and $\\mu$ is the rate of death.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Length of a queue in a Bank\n",
    "\n",
    "Embedded random walk from a birth-death process from a birth death process is constructed by examining the b-d process only at the transition times. This creates a discrete time MC: $\\bc{Y_n}_{n=0}^\\infty$, $Y_0=X(0)$.\n",
    "\n",
    "For a general BD process $Y_n$ has a transition prob matrix of \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & & ... \\\\\n",
    "q_1 & 0  & p_1 & 0 ... \\\\\n",
    "0 & q_2  & 0 & p_2 ... \\\\\n",
    "\\vdots & \\ddots \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$q_i = \\frac{\\mu_i}{\\mu_i+\\lambda_i}$,  $p_i = \\frac{\\lambda_i}{\\mu_i+\\lambda_i}$\n",
    "\n",
    "$u_i$ is prob of extinction starting from state $i$.  \n",
    "This problem is similar ro a gamblers ruin problem.\n",
    "\n",
    "Start with $i$ people $\\leftrightarrow$ start with \\$$i$ \n",
    "\n",
    "Remark: Discretization of the continuous MC is not unique to the birth-death process. It can be done in the simple beirth-death process as well. An important advantage of casting the phenomenon using the discrete chain is that it can give you extinction probabilities with minimal calculation.\n",
    "\n",
    "The solutions will be the same as in the continuous version of the SP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with discrete MC\n",
    "\n",
    "$X_1=x_1,...,X_m=x_m$.\n",
    "Let the chain have a transition prob matrix of $K\\times K$ with $i,j$ entry $=p_{ij}$.\n",
    "\n",
    "Then $L(p) = \\prod_i \\prod_j p_{ij}^{n_{ij}}$ where $n_{ij}$ is the number of transitions from $i$ to $j$ and the sum of the $n_{ij}$ is $m$.\n",
    "\n",
    "Prior on $p_{ij}$: $p_i \\sim Dir(\\alpha_{i1},...,\\alpha_{iK})$\n",
    "Posterior $p_{ij}$: $p_i|X \\sim Dir(\\alpha_{i1}',...,\\alpha_{iK}')$\n",
    "\n",
    "### Rainfall data: weatherzone.com.au\n",
    "Occurence on non-occurence of rain in the desert areas of Australia. If rainfall occurs in a month the MC is assumed to be in state 1. If it doesn't occur, it is at state 0.\n",
    "\n",
    "### Hidden Markov Models\n",
    "Observation $Y_n$ for $n=0,1,...$ are generated from a conditional distribution $f(y_n|x_n)$ with parameters depending on an unnobserved or hidden state, $x_n \\in \\bc{1,2,...,K}$. Hidden states follow a transition matrix $P$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
